{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05373b1b-ef95-449c-89c7-40069b5588ea",
   "metadata": {},
   "source": [
    "# Assignment 4: Text Classification on TREC dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2ad257-ec97-47bd-8cca-a2655fa5d92e",
   "metadata": {},
   "source": [
    "We are going to use the TREC dataset for this assignment, which is widely considered a benchmark text classification dataset. Read about the TREC dataset here (https://huggingface.co/datasets/CogComp/trec), also google it for understanding it better.\n",
    "\n",
    "This is what you have to do - use the concepts we have covered so far to accurately predict the 5 coarse labels (if you have googled TERC, you will surely know what I mean) in the test dataset. Train on the train dataset and give results on the test dataset, as simple as that. And experiment, experiment and experiment! \n",
    "\n",
    "Your experimentation should be 4-tiered-\n",
    "\n",
    "i) Experiment with preprocessing techniques (different types of Stemming, Lemmatizing, or do neither and keep the words pure). Needless to say, certain things, like stopword removal, should be common in all the preprocesssing pipelines you come up with. Remember never do stemming and lemmatization together. Note - To find out the best preprocessing technique, use a simple baseline model, like say CountVectorizer(BoW) + Logistic Regression, and see which gives the best accuracy. Then proceed with that preprocessing technique only for all the other models.\n",
    "\n",
    "ii) Try out various vectorisation techniques (BoW, TF-IDF, CBoW, Skipgram, GloVE, Fasttext, etc., but transformer models are not allowed) -- Atleast 5 different types\n",
    "\n",
    "iii) Tinker with various strategies to combine the word vectors (taking mean, using RNN/LSTM, and the other strategies I hinted at in the end of the last sesion). Note that this is applicable only for the advanced embedding techniques which generate word embeddings. -- Atleast 3 different types, one of which should definitely be RNN/LSTM\n",
    "\n",
    "iv) Finally, experiment with the ML classifier model, which will take the final vector respresentation of each TREC question and generate the label. E.g. - Logistic regression, decision trees, simple neural network, etc. - Atleast 4 different models\n",
    "\n",
    "So applying some PnC, in total you should get more than 40 different combinations. Print out the accuracies of all these combinations nicely in a well-formatted table, and pronounce one of them the best. Also feel free to experiment with more models/embedding techniques than what I have said here, the goal is after all to achieve the highest accuracy, as long as you don't use transformers. Happy experimenting!\n",
    "\n",
    "NOTE - While choosing the 4-5 types of each experimentation level, try to choose the best out of all those available. E.g. - For level (iii) - Tinker with various strategies to combine the word vectors - do not include 'mean' if you see it is giving horrendous results. Include the best 3-4 strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cca5a12-6ddf-4895-a962-fd8fac4ad1f9",
   "metadata": {},
   "source": [
    "### Helper Code to get you started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d08592-c633-4764-a60a-4937fd768cb4",
   "metadata": {},
   "source": [
    "I have added some helper code to show you how to load the TERC dataset and use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d25cda3c-7d29-42c5-82b1-17ff2ac0d1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9659079-4080-498b-a4a2-85568f599b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Question: How did serfdom develop in and then leave Russia ?\n",
      "Label: 2\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"trec\", trust_remote_code=True)\n",
    "train_data = dataset['train']\n",
    "test_data = dataset['test']\n",
    "\n",
    "print(\"Sample Question:\", train_data[0]['text'])\n",
    "print(\"Label:\", train_data[0]['coarse_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "152ff309-ba4d-4ed9-b734-6bfcede7c076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nv909\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nv909\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\nv909\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from datasets import load_dataset\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import gensim.downloader as api\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d383ad6-b6aa-40b6-8162-16f0b4017ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>coarse_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How did serfdom develop in and then leave Russ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What films featured the character Popeye Doyle ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I find a list of celebrities ' real na...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What fowl grabs the spotlight after the Chines...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the full form of .com ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  coarse_label\n",
       "0  How did serfdom develop in and then leave Russ...             2\n",
       "1   What films featured the character Popeye Doyle ?             1\n",
       "2  How can I find a list of celebrities ' real na...             2\n",
       "3  What fowl grabs the spotlight after the Chines...             1\n",
       "4                    What is the full form of .com ?             0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df= train_data.to_pandas()\n",
    "test_df = test_data.to_pandas()\n",
    "train_df.drop(columns =['fine_label'],inplace = True)\n",
    "test_df.drop(columns =['fine_label'],inplace = True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de69fe12-b553-4e58-8354-456affede0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>coarse_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How far is it from Denver to Aspen ?</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What county is Modesto , California in ?</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who was Galileo ?</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is an atom ?</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When did Hawaii become a state ?</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       text  coarse_label\n",
       "0      How far is it from Denver to Aspen ?             5\n",
       "1  What county is Modesto , California in ?             4\n",
       "2                         Who was Galileo ?             3\n",
       "3                         What is an atom ?             2\n",
       "4          When did Hawaii become a state ?             5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25e1d17e-1930-4b44-a330-d55965077126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "coarse_label\n",
       "1    1250\n",
       "3    1223\n",
       "2    1162\n",
       "5     896\n",
       "4     835\n",
       "0      86\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['coarse_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b7ce869-f689-4940-9556-0e7f828ce382",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer =PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b6b652b-4adf-4c76-8c23-9f8696a07c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, method ='none'):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    if method =='stem':\n",
    "        tokens = [stemmer.stem(t) for t in tokens]\n",
    "    elif method == 'lemma':\n",
    "        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    else:\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33c8df6c-d9f7-4c19-91ae-481d7b12f0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
      "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "glove_embeddings = api.load(\"glove-wiki-gigaword-100\")\n",
    "cbow_embeddings = api.load(\"word2vec-google-news-300\")\n",
    "fasttext_embeddings = api.load(\"fasttext-wiki-news-subwords-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5041c219-920d-4fbf-8e20-35dfd9ffcb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df['lemmatized_text'] = train_df['text'].apply(lambda x: preprocess(x, method = 'lemma'))\n",
    "test_df['lemmatized_text'] = train_df['text'].apply(lambda x: preprocess(x, method = 'lemma'))\n",
    "train_df['stemmed_text'] = train_df['text'].apply(lambda x: preprocess(x, method='stem'))\n",
    "test_df['stemmed_text'] = test_df['text'].apply(lambda x: preprocess(x, method='stem'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "948f3004-679c-4e01-abcb-669492e02c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text_with_model(corpus, embeddings,type='mean'):\n",
    "    embedded = []\n",
    "    for sentence in corpus:\n",
    "        words = sentence.split()\n",
    "        vectors = [embeddings[word] for word in words if word in embeddings]\n",
    "        if vectors:\n",
    "            if type == 'mean':\n",
    "                embedded.append(np.mean(vectors, axis=0))\n",
    "            elif type == 'max_pooling':\n",
    "                embedded.append(np.max(vectors, axis=0))\n",
    "        else:\n",
    "            embedded.append(np.zeros(embeddings.vector_size))\n",
    "    return np.array(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf3de71b-d787-4853-b6a4-86b29705ee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(method, train, test,type='mean'):\n",
    "    if method == 'BoW':\n",
    "        vectorizer = CountVectorizer()\n",
    "        return vectorizer.fit_transform(train), vectorizer.transform(test)\n",
    "    elif method == 'TF-IDF':\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        return vectorizer.fit_transform(train), vectorizer.transform(test)\n",
    "    elif method == 'CBoW':\n",
    "        embeddings = cbow_embeddings\n",
    "        return embed_text_with_model(train, embeddings,type), embed_text_with_model(test, embeddings,type)\n",
    "    elif method == 'GloVe':\n",
    "        embeddings = glove_embeddings\n",
    "        return embed_text_with_model(train, embeddings,type), embed_text_with_model(test, embeddings,type)\n",
    "    elif method == 'FastText':\n",
    "        embeddings = fasttext_embeddings\n",
    "        return embed_text_with_model(train, embeddings,type), embed_text_with_model(test, embeddings,type)\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c03e658e-5308-42bd-878b-f5117cda6f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train= train_df['coarse_label']\n",
    "y_test = test_df['coarse_label']\n",
    "vectorization_methods = ['BoW', 'TF-IDF','CBoW', 'GloVe', 'FastText']\n",
    "preprocessing_methods = ['lemma', 'stem','none']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6209e31c-18e8-429a-a3af-3a3ecafc73bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d66e6298-1ad6-4e92-b3f9-0a00dbd15d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100),\n",
    "    'SVM': SVC()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd7b8b22-a49c-4437-9b4a-4b580a33ec7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing: lemma\n",
      "\n",
      "Preprocessing: stem\n",
      "\n",
      "Preprocessing: none\n"
     ]
    }
   ],
   "source": [
    "baseline_results = []\n",
    "\n",
    "for preprocessing in preprocessing_methods:\n",
    "    print(f\"Preprocessing-{preprocessing}\")\n",
    "    if preprocessing == 'lemma':\n",
    "        train_texts = train_df['lemmatized_text']\n",
    "        test_texts = test_df['lemmatized_text']\n",
    "    elif preprocessing == 'stem':\n",
    "        train_texts = train_df['stemmed_text']\n",
    "        test_texts = test_df['stemmed_text']\n",
    "    else:\n",
    "        train_texts = train_df['text']\n",
    "        test_texts = test_df['text']\n",
    "\n",
    "    X_train, X_test = vectorize('BoW', train_texts, test_texts)\n",
    "\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    acc = evaluate(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    baseline_results.append({\n",
    "        'Preprocessing': preprocessing,\n",
    "        'Vectorizer': 'BoW',\n",
    "        'Model': 'Logistic Regression',\n",
    "        'Accuracy': acc,\n",
    "        'Combination': \"Mean\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3218a7b6-091f-4e37-bb11-285f0a7df35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline Tests:\n",
      "  Preprocessing Vectorizer                Model  Accuracy Combination\n",
      "0          none        BoW  Logistic Regression     0.852        Mean\n",
      "1          stem        BoW  Logistic Regression     0.754        Mean\n",
      "2         lemma        BoW  Logistic Regression     0.208        Mean\n"
     ]
    }
   ],
   "source": [
    "baseline = pd.DataFrame(baseline_results)\n",
    "print(\"\\nBaseline Tests:\")\n",
    "print(baseline.sort_values(by='Accuracy', ascending=False).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99138719-0005-4360-ba13-40af99882744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vectorization-BoW\n",
      "Model-Logistic Regression\n",
      "Model-Decision Tree\n",
      "Model-Random Forest\n",
      "Model-SVM\n",
      "\n",
      "Vectorization-TF-IDF\n",
      "Model-Logistic Regression\n",
      "Model-Decision Tree\n",
      "Model-Random Forest\n",
      "Model-SVM\n",
      "\n",
      "Vectorization-CBoW\n",
      "Model-Logistic Regression\n",
      "Model-Decision Tree\n",
      "Model-Random Forest\n",
      "Model-SVM\n",
      "\n",
      "Vectorization-GloVe\n",
      "Model-Logistic Regression\n",
      "Model-Decision Tree\n",
      "Model-Random Forest\n",
      "Model-SVM\n",
      "\n",
      "Vectorization-FastText\n",
      "Model-Logistic Regression\n",
      "Model-Decision Tree\n",
      "Model-Random Forest\n",
      "Model-SVM\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for vec_method in vectorization_methods:\n",
    "    print(f\"\\nVectorization-{vec_method}\")\n",
    "    X_train, X_test = vectorize(vec_method, train_df['text'], test_df['text'],type='max_pooling')\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Model-{model_name}\")\n",
    "        acc = evaluate(model, X_train, y_train, X_test, y_test)\n",
    "        results.append({\n",
    "            'Preprocessing': 'None',\n",
    "            'Vectorizer': vec_method,\n",
    "            'Model': model_name,\n",
    "            'Accuracy': acc,\n",
    "            'Combination': \"Max Pooling\"\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9500f7ee-f3da-49ab-bd36-667f484cbe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Masking\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a67b0d0a-f3c6-4241-a600-e21a76abdbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['clean_text'] = train_df['text'].apply(lambda x: preprocess(x, method='none'))\n",
    "test_df['clean_text'] = test_df['text'].apply(lambda x: preprocess(x, method='none'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de9bbd65-98de-407d-be67-1b96bb7922fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_to_sequences(texts, embeddings, dim):\n",
    "    sequences = []\n",
    "    for sentence in texts:\n",
    "        tokens = sentence.split()\n",
    "        vecs = [embeddings[w] if w in embeddings else np.zeros(dim) for w in tokens]\n",
    "        sequences.append(vecs)\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72cf7409-e86e-450b-bfe9-97210d29130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {\n",
    "    'Word2Vec': cbow_embeddings,\n",
    "    'GloVe': glove_embeddings,\n",
    "    'FastText': fasttext_embeddings\n",
    "}\n",
    "embedding_dims = {\n",
    "    'Word2Vec': 300,\n",
    "    'GloVe': 100,\n",
    "    'FastText': 300\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9c0903d-3de5-4e34-8963-25f48341b9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cat = to_categorical(y_train, 6)\n",
    "y_test_cat = to_categorical(y_test, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f515d90c-5aad-45d3-8235-d20ef4f4d34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using embedding: Word2Vec\n",
      "Epoch 1/5\n",
      "137/137 [==============================] - 23s 76ms/step - loss: 1.2769 - accuracy: 0.5224 - val_loss: 0.9556 - val_accuracy: 0.6691\n",
      "Epoch 2/5\n",
      "137/137 [==============================] - 5s 38ms/step - loss: 0.8585 - accuracy: 0.7044 - val_loss: 0.8025 - val_accuracy: 0.7241\n",
      "Epoch 3/5\n",
      "137/137 [==============================] - 5s 33ms/step - loss: 0.7035 - accuracy: 0.7585 - val_loss: 0.7490 - val_accuracy: 0.7351\n",
      "Epoch 4/5\n",
      "137/137 [==============================] - 5s 37ms/step - loss: 0.6099 - accuracy: 0.7939 - val_loss: 0.7791 - val_accuracy: 0.7324\n",
      "Epoch 5/5\n",
      "137/137 [==============================] - 5s 36ms/step - loss: 0.5279 - accuracy: 0.8195 - val_loss: 0.7445 - val_accuracy: 0.7507\n",
      "16/16 [==============================] - 4s 13ms/step\n",
      "\n",
      "Using embedding: GloVe\n",
      "Epoch 1/5\n",
      "137/137 [==============================] - 18s 56ms/step - loss: 1.2930 - accuracy: 0.4928 - val_loss: 1.0093 - val_accuracy: 0.6343\n",
      "Epoch 2/5\n",
      "137/137 [==============================] - 5s 38ms/step - loss: 0.9525 - accuracy: 0.6471 - val_loss: 0.9037 - val_accuracy: 0.6801\n",
      "Epoch 3/5\n",
      "137/137 [==============================] - 6s 41ms/step - loss: 0.8128 - accuracy: 0.7102 - val_loss: 0.8659 - val_accuracy: 0.6994\n",
      "Epoch 4/5\n",
      "137/137 [==============================] - 4s 31ms/step - loss: 0.7247 - accuracy: 0.7413 - val_loss: 0.8389 - val_accuracy: 0.7067\n",
      "Epoch 5/5\n",
      "137/137 [==============================] - 5s 35ms/step - loss: 0.6328 - accuracy: 0.7845 - val_loss: 0.8520 - val_accuracy: 0.7186\n",
      "16/16 [==============================] - 5s 13ms/step\n",
      "\n",
      "Using embedding: FastText\n",
      "Epoch 1/5\n",
      "137/137 [==============================] - 19s 64ms/step - loss: 1.3960 - accuracy: 0.4634 - val_loss: 1.0872 - val_accuracy: 0.6489\n",
      "Epoch 2/5\n",
      "137/137 [==============================] - 5s 39ms/step - loss: 1.0406 - accuracy: 0.6556 - val_loss: 0.9587 - val_accuracy: 0.6847\n",
      "Epoch 3/5\n",
      "137/137 [==============================] - 5s 38ms/step - loss: 0.9095 - accuracy: 0.6955 - val_loss: 0.8913 - val_accuracy: 0.6966\n",
      "Epoch 4/5\n",
      "137/137 [==============================] - 4s 33ms/step - loss: 0.8187 - accuracy: 0.7182 - val_loss: 0.8273 - val_accuracy: 0.7030\n",
      "Epoch 5/5\n",
      "137/137 [==============================] - 5s 39ms/step - loss: 0.7535 - accuracy: 0.7400 - val_loss: 0.8401 - val_accuracy: 0.7122\n",
      "16/16 [==============================] - 5s 22ms/step\n"
     ]
    }
   ],
   "source": [
    "for name, embedding in embeddings.items():\n",
    "    print(f\"\\nUsing embedding: {name}\")\n",
    "    dim = embedding_dims[name]\n",
    "    train_seqs = texts_to_sequences(train_df['clean_text'], embedding, dim=dim)\n",
    "    test_seqs = texts_to_sequences(test_df['clean_text'], embedding, dim=dim)\n",
    "\n",
    "    maxlen = max(max(len(s) for s in train_seqs), max(len(s) for s in test_seqs))\n",
    "    X_train_pad = pad_sequences(train_seqs, maxlen=maxlen, dtype='float32', padding='post', truncating='post')\n",
    "    X_test_pad = pad_sequences(test_seqs, maxlen=maxlen, dtype='float32', padding='post', truncating='post')\n",
    "\n",
    "    input = Input(shape=(maxlen, dim))\n",
    "    x = Masking(mask_value=0.0)(input)\n",
    "    x = LSTM(128)(x) \n",
    "    output = Dense(6, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=input, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train_pad, y_train_cat, epochs=5, batch_size=32, validation_split=0.2)\n",
    "    y_pred = model.predict(X_test_pad)\n",
    "    acc = accuracy_score(np.argmax(y_test_cat, axis=1), np.argmax(y_pred, axis=1))\n",
    "    results.append({\n",
    "            'Preprocessing': 'None',\n",
    "            'Vectorizer': name,\n",
    "            'Model': 'LSTM',\n",
    "            'Accuracy': acc,\n",
    "            'Combination': \" \"\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8da14a07-1f00-4e78-8925-bd26b7a6cfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Preprocessing Vectorizer                Model  Accuracy  Combination\n",
      "0           None     TF-IDF                  SVM     0.864  Max Pooling\n",
      "1           None        BoW  Logistic Regression     0.852  Max Pooling\n",
      "2           None     TF-IDF  Logistic Regression     0.852  Max Pooling\n",
      "3           None     TF-IDF        Random Forest     0.838  Max Pooling\n",
      "4           None        BoW        Random Forest     0.836  Max Pooling\n",
      "5           None        BoW                  SVM     0.836  Max Pooling\n",
      "6           None       CBoW                  SVM     0.834  Max Pooling\n",
      "7           None        BoW        Decision Tree     0.828  Max Pooling\n",
      "8           None       CBoW        Random Forest     0.792  Max Pooling\n",
      "9           None   FastText        Random Forest     0.786  Max Pooling\n",
      "10          None     TF-IDF        Decision Tree     0.766  Max Pooling\n",
      "11          None   FastText  Logistic Regression     0.760  Max Pooling\n",
      "12          None       CBoW  Logistic Regression     0.756  Max Pooling\n",
      "13          None   FastText                  SVM     0.752  Max Pooling\n",
      "14          None      GloVe        Random Forest     0.742  Max Pooling\n",
      "15          None      GloVe                  SVM     0.672  Max Pooling\n",
      "16          None   FastText        Decision Tree     0.640  Max Pooling\n",
      "17          None      GloVe        Decision Tree     0.606  Max Pooling\n",
      "18          None      GloVe  Logistic Regression     0.602  Max Pooling\n",
      "19          None       CBoW        Decision Tree     0.596  Max Pooling\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.sort_values(by='Accuracy', ascending=False).reset_index(drop=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
