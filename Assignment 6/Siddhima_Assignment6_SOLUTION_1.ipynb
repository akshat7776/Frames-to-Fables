{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "11623f8b",
      "metadata": {
        "id": "11623f8b"
      },
      "source": [
        "## Assignment 6\n",
        "This assignment will explore some of the basic and intutive methods that we can use to generate a story out of a sequence of images.\n",
        "\n",
        "The plot is like we will first gnerate detailed captions for each image, then we will use these captions to generate a story using a language model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f14fefb2",
      "metadata": {
        "id": "f14fefb2"
      },
      "source": [
        "#### Image captioning\n",
        "We will use a pre-trained model to generate captions for each image.\n",
        "You have to experiment with different open-sorce image captioning models document and store the results of the captions generated by each model in a file. Update the advantages and disadvantages of each model in the google doc.\n",
        "\n",
        "For sample you can use the following four images:\n",
        "\n",
        "<div style=\"display: grid; grid-template-columns: repeat(2, 1fr); gap: 10px;\">\n",
        "<img src=\"https://i.ibb.co/RGH8hK6n/Screenshot-2025-06-26-145250.png\" alt=\"Image 1\" width=\"300\">\n",
        "\n",
        "<img src=\"https://i.ibb.co/Mk4mjJhj/Screenshot-2025-06-26-145414.png\" alt=\"Image 2\" width=\"300\">\n",
        "\n",
        "<img src=\"https://i.ibb.co/jvVhChFp/Screenshot-2025-06-26-145429.png\" alt=\"Image 3\" width=\"300\">\n",
        "\n",
        "<img src=\"https://i.ibb.co/8gbYcM0d/Screenshot-2025-06-26-145506.png\" alt=\"Image 4\" width=\"300\">\n",
        "</div>\n",
        "\n",
        "https://i.ibb.co/RGH8hK6n/Screenshot-2025-06-26-145250.png\n",
        "https://i.ibb.co/Mk4mjJhj/Screenshot-2025-06-26-145414.png\n",
        "https://i.ibb.co/jvVhChFp/Screenshot-2025-06-26-145429.png\n",
        "https://i.ibb.co/8gbYcM0d/Screenshot-2025-06-26-145506.png\n",
        "\n",
        "You can also use images of your choice, but keep them same for all the models you are trying, so that you can compare the results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6a53d7b",
      "metadata": {
        "id": "a6a53d7b"
      },
      "source": [
        "You have to try atleast 6-7 different models for image captioning, you may choose from the models available on HuggingFace or any other source. Don't stop until you find a model that gives you desired results.\n",
        "\n",
        "You may try variety of models including those based on CLIP, BLIP, Vision-Language Models, Visual Transfrmers, etc. (These are variuos multimodal architectures, you read more about their architectures, uses, etc. online). You can simply google \"image captioning models HuggingFace\" based on any architecture (CLIP, BLIP, VLM, etc.) and you will find many models that you can try.\n",
        "You may also look into their benchmark results to choose most suitable even before trying them out.\n",
        "In my opinion, Vision-Language Models (VLMs) are the most suitable considering the amount of details we want in the captions, but you should try others too.\n",
        "\n",
        "Note: It's ok if you don't completely understand architectures of the models you use, but you should try to understand the basic idea of how they work and if you have further doubts you can reach out to me.\n",
        "Same goes for the code that you will use to generate the captions, you can view how to use the model in the documentation of that particular model, but try to understand the meaning of all the compenents of your code.\n",
        "You may use AI tools to generate the code, but make sure to understand every part of it, else it beats the purpose.\n",
        "Also copy paste the code you used for all the models in this notebook for submission.\n",
        "\n",
        "Also most of the image captioning models on HuggingFace will be large and computationally expensive, so you should not try to run them locally, you can use Goggle Colab, or Kaggle notebooks to run models on their servers (Kaggle provides free GPU for 30 hours per month, so I think that might be enough, even if it's not, that's why I made teams for this assignment, so that you can divide the GPU time across your team members).\n",
        "\n",
        "Another important point is that, some models will be even larger that the computational resources provided by Kaggle or Google colab, so in that case you will have to load the model in quantized format, which will reduce the size of the model and make it easier to run on limited resources.\n",
        "\n",
        "Quantization: It is a technique used to reduce the size of a model by converting its weights from floating-point precision to lower precision, for eg. if originally the weights are in 32-bit floating point format, then you may convert them to 16 or 8 or even 4-bit format. This reduces the memory footprint of the model and makes it easier to run on limited resources. However this may also reduce the performance of the model, so you should try to find a balance between the size and performance of the model.\n",
        "\n",
        "For loading a model in quantized format, there is a llibrary called `bitsandbytes`, for more details on how to use it, you can refer to online resources or ChatGPT, etc.\n",
        "Example code:\n",
        "```python\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
        "    \"bigscience/bloom-1b7\",\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5f29dba",
      "metadata": {
        "id": "d5f29dba"
      },
      "source": [
        "Here is playlist that I found helpful for understanding huggingface, you may watch the videos that you find helpful:\n",
        "[HuggingFace Playlist](https://www.youtube.com/playlist?list=PLo2EIpI_JMQvWfQndUesu0nPBAtZ9gP1o)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fae32ec7",
      "metadata": {
        "id": "fae32ec7"
      },
      "outputs": [],
      "source": [
        "# Code for image captioning\n",
        "# Feel free to use Kaggle for actually running the code, but paste it here after you are done.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# model 1\n",
        "# Model name: BLIP\n",
        "# Link:\n",
        "# Code:\n",
        "!pip install transformers==4.40.1\n",
        "!pip install torch torchvision\n",
        "!pip install accelerate\n",
        "!pip install Pillow\n",
        "\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "import os\n",
        "\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def generate_caption(image):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
        "    output = model.generate(**inputs)\n",
        "    caption = processor.decode(output[0], skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "image_urls = [\n",
        "    \"https://www.google.com/url?q=https%3A%2F%2Fi.ibb.co%2FRGH8hK6n%2FScreenshot-2025-06-26-145250.png\",\n",
        "    \"https://www.google.com/url?q=https%3A%2F%2Fi.ibb.co%2FMk4mjJhj%2FScreenshot-2025-06-26-145414.png\",\n",
        "    \"https://www.google.com/url?q=https%3A%2F%2Fi.ibb.co%2FjvVhChFp%2FScreenshot-2025-06-26-145429.png\",\n",
        "    \"https://www.google.com/url?q=https%3A%2F%2Fi.ibb.co%2F8gbYcM0d%2FScreenshot-2025-06-26-145506.png\"\n",
        "]\n",
        "\n",
        "import urllib.parse\n",
        "\n",
        "for idx, url in enumerate(image_urls):\n",
        "    # Extract the actual image URL from the Google redirect link\n",
        "    parsed_url = urllib.parse.urlparse(url)\n",
        "    actual_image_url = urllib.parse.parse_qs(parsed_url.query)['q'][0]\n",
        "\n",
        "    image = Image.open(requests.get(actual_image_url, stream=True).raw).convert(\"RGB\")\n",
        "    caption = generate_caption(image)\n",
        "    print(f\"Image {idx+1} Caption: {caption}\")\n",
        "\n",
        "# Image 1 Caption: a cartoon of two children playing with a toy\n",
        "# Image 2 Caption: a cartoon of a woman and a child playing with a toy\n",
        "# Image 3 Caption: a cartoon of two children sitting at a table\n",
        "# Image 4 Caption: a cartoon of a family playing with a paper airplane\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# model 2\n",
        "# Model name: BLIP2\n",
        "# Link:\n",
        "# Code:\n",
        "!pip install transformers accelerate\n",
        "!pip install torch torchvision\n",
        "!pip install bitsandbytes\n",
        "!pip install Pillow\n",
        "\n",
        "import torch\n",
        "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "model_name = \"Salesforce/blip2-flan-t5-xl\"\n",
        "\n",
        "processor = Blip2Processor.from_pretrained(model_name)\n",
        "model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=torch.float16)\n",
        "\n",
        "!pip install --upgrade transformers\n",
        "\n",
        "def generate_blip2_caption(image):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    inputs = processor(images=image, return_tensors=\"pt\").to(device, torch.float16)\n",
        "    output = model.generate(**inputs, max_new_tokens=50)\n",
        "    caption = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "image_urls = [\n",
        "    \"https://www.google.com/url?q=https%3A%2F%2Fi.ibb.co%2FRGH8hK6n%2FScreenshot-2025-06-26-145250.png\",\n",
        "    \"https://www.google.com/url?q=https%3A%2F%2Fi.ibb.co%2FMk4mjJhj%2FScreenshot-2025-06-26-145414.png\",\n",
        "    \"https://www.google.com/url?q=https%3A%2F%2Fi.ibb.co%2FjvVhChFp%2FScreenshot-2025-06-26-145429.png\",\n",
        "    \"https://www.google.com/url?q=https%3A%2F%2Fi.ibb.co%2F8gbYcM0d%2FScreenshot-2025-06-26-145506.png\"\n",
        "]\n",
        "\n",
        "import urllib.parse\n",
        "\n",
        "for idx, url in enumerate(image_urls):\n",
        "    # Extract the actual image URL from the Google redirect link\n",
        "    parsed_url = urllib.parse.urlparse(url)\n",
        "    actual_image_url = urllib.parse.parse_qs(parsed_url.query)['q'][0]\n",
        "\n",
        "    image = Image.open(requests.get(actual_image_url, stream=True).raw).convert(\"RGB\")\n",
        "    caption = generate_blip2_caption(image)\n",
        "    print(f\"Image {idx+1} Caption: {caption}\")\n",
        "\n",
        "# Image 1 Caption: a cartoon of a boy and girl playing with toys\n",
        "# Image 2 Caption: a cartoon of a mother and child playing with a toy\n",
        "# Image 3 Caption: a boy and girl are playing with paper airplanes\n",
        "# Image 4 Caption: a girl is playing with her children in the kitchen\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# model 3\n",
        "# Model name: GIT\n",
        "# Link:\n",
        "# Code:\n",
        "!pip install transformers\n",
        "!pip install torch torchvision\n",
        "!pip install Pillow\n",
        "\n",
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "from PIL import Image\n",
        "import torch\n",
        "import requests\n",
        "\n",
        "model_name = \"microsoft/git-base\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def generate_git_caption(image):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
        "\n",
        "    # Add prompt for caption generation\n",
        "    input_ids = processor(text=\"describe the image\", return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "    # Generate caption\n",
        "    generated_ids = model.generate(pixel_values=pixel_values, input_ids=input_ids, max_length=50)\n",
        "    caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    return caption\n",
        "\n",
        "image_urls = [\n",
        "    \"https://www.google.com/url?q=https%3A%2F%2Fi.ibb.co%2FRGH8hK6n%2FScreenshot-2025-06-26-145250.png\",\n",
        "    \"https://www.google.com/url?q=https%3A%2F%2Fi.ibb.co%2FMk4mjJhj%2FScreenshot-2025-06-26-145414.png\",\n",
        "    \"https://www.google.com/url?q=https%3A%2F%2Fi.ibb.co%2FjvVhChFp%2FScreenshot-2025-06-26-145429.png\",\n",
        "    \"https://www.google.com/url?q=https%3A%2F%2Fi.ibb.co%2F8gbYcM0d%2FScreenshot-2025-06-26-145506.png\"\n",
        "]\n",
        "\n",
        "import urllib.parse\n",
        "\n",
        "for idx, url in enumerate(image_urls):\n",
        "    # Extract the actual image URL from the Google redirect link\n",
        "    parsed_url = urllib.parse.urlparse(url)\n",
        "    actual_image_url = urllib.parse.parse_qs(parsed_url.query)['q'][0]\n",
        "\n",
        "    image = Image.open(requests.get(actual_image_url, stream=True).raw).convert(\"RGB\")\n",
        "    caption = generate_git_caption(image)\n",
        "    print(f\"Image {idx+1} Caption: {caption}\")\n",
        "\n",
        "# Image 1 Caption: describe the image the child in the house\n",
        "# Image 2 Caption: describe the image the girl in the red dress is the one in the picture.\n",
        "# Image 3 Caption: describe the image the boy in the yellow shirt.\n",
        "# Image 4 Caption: describe the image the boy in the blue shirt and the girl in the background is the boy.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# model 4\n",
        "# Model name:ViLT\n",
        "# Link:\n",
        "# Code:\n",
        "!pip install transformers\n",
        "!pip install torch torchvision\n",
        "!pip install Pillow\n",
        "\n",
        "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "\n",
        "model_name = \"dandelin/vilt-b32-finetuned-vqa\"\n",
        "\n",
        "processor = ViltProcessor.from_pretrained(model_name)\n",
        "model = ViltForQuestionAnswering.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def vilt_caption_style(image, question=\"What is in the image?\"):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    inputs = processor(image, question, return_tensors=\"pt\").to(device)\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    predicted_id = logits.argmax(-1).item()\n",
        "    answer = model.config.id2label[predicted_id]\n",
        "    return answer\n",
        "\n",
        "image_urls = [\n",
        "    \"https://www.google.com/url?q=https%3A%2F%2Fi.ibb.co%2FRGH8hK6n%2FScreenshot-2025-06-26-145250.png\",\n",
        "    \"https://www.google.com/url?q=https%3A%2F%2Fi.ibb.co%2FMk4mjJhj%2FScreenshot-2025-06-26-145414.png\",\n",
        "    \"https://www.google.com/url?q=https%3A%2F%2Fi.ibb.co%2FjvVhChFp%2FScreenshot-2025-06-26-145429.png\",\n",
        "    \"https://www.google.com/url?q=https%3A%2F%2Fi.ibb.co%2F8gbYcM0d%2FScreenshot-2025-06-26-145506.png\"\n",
        "]\n",
        "\n",
        "import urllib.parse\n",
        "\n",
        "for idx, url in enumerate(image_urls):\n",
        "    # Extract the actual image URL from the Google redirect link\n",
        "    parsed_url = urllib.parse.urlparse(url)\n",
        "    actual_image_url = urllib.parse.parse_qs(parsed_url.query)['q'][0]\n",
        "\n",
        "    image = Image.open(requests.get(actual_image_url, stream=True).raw).convert(\"RGB\")\n",
        "    caption = vilt_caption_style(image)\n",
        "    print(f\"Image {idx+1} Caption (VQA-style): {caption}\")\n",
        "\n",
        "# Image 1 Caption (VQA-style): toothbrush\n",
        "# Image 2 Caption (VQA-style): kids\n",
        "# Image 3 Caption (VQA-style): kids\n",
        "# Image 4 Caption (VQA-style): kids\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# model 5\n",
        "# Model name:InstructBLIP\n",
        "# Link:\n",
        "# Code:\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install torch torchvision\n",
        "!pip install bitsandbytes\n",
        "!pip install Pillow\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "import requests\n",
        "from transformers import InstructBlipProcessor, InstructBlipForConditionalGeneration\n",
        "\n",
        "model_id = \"Salesforce/instructblip-flan-t5-xl\"\n",
        "\n",
        "processor = InstructBlipProcessor.from_pretrained(model_id)\n",
        "model = InstructBlipForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    load_in_8bit=True  # saves memory, works well on T4\n",
        ")\n",
        "\n",
        "def generate_instructblip_caption(image, instruction=\"Describe the image in detail.\"):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    inputs = processor(images=image, text=instruction, return_tensors=\"pt\").to(device, torch.float16)\n",
        "    output = model.generate(**inputs, max_new_tokens=300)\n",
        "    caption = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "image_urls = [\n",
        "    \"https://www.google.com/url?q=https%3A%2F%2Fi.ibb.co%2FRGH8hK6n%2FScreenshot-2025-06-26-145250.png\",\n",
        "    \"https://www.google.com/url?q=https%3A%2F%2Fi.ibb.co%2FMk4mjJhj%2FScreenshot-2025-06-26-145414.png\",\n",
        "    \"https://www.google.com/url?q=https%3A%2F%2Fi.ibb.co%2FjvVhChFp%2FScreenshot-2025-06-26-145429.png\",\n",
        "    \"https://www.google.com/url?q=https%3A%2F%2Fi.ibb.co%2F8gbYcM0d%2FScreenshot-2025-06-26-145506.png\"\n",
        "]\n",
        "\n",
        "import urllib.parse\n",
        "\n",
        "for idx, url in enumerate(image_urls):\n",
        "    # Extract the actual image URL from the Google redirect link\n",
        "    parsed_url = urllib.parse.urlparse(url)\n",
        "    actual_image_url = urllib.parse.parse_qs(parsed_url.query)['q'][0]\n",
        "\n",
        "    image = Image.open(requests.get(actual_image_url, stream=True).raw).convert(\"RGB\")\n",
        "    caption = generate_instructblip_caption(image)\n",
        "    print(f\"Image {idx+1} Caption: {caption}\")\n",
        "\n",
        "# Image 1 Caption: The image features a cartoon depicting a family in a living room, with two children playing with a toy airplane. The children are interacting with each other, with one of them holding the toy airplane in his hands. The other child is positioned in the middle of the room, with the other two children positioned in the corners. The image is part of a cartoon series, which depicts a family in a living room, with a toy airplane in one hand and a toy car in the other. The cartoon is part of a series of illustrations, which depict a family in a living room, with a toy airplane in one hand and a toy car in the other.\n",
        "\n",
        "# Image 2 Caption: The image features a cartoon of a family in a living room, with a mother and two children in the background. The mother is holding a small airplane, while the children are playing with it. The children are interacting with each other, with one of them pointing to the airplane, while the other one is pointing to the woman. The woman is wearing a red shirt, while the man is wearing a blue shirt. The children are positioned in different parts of the room, with one of them pointing to the airplane, while the other one is pointing to the woman. The image is part of a cartoon series, which depicts a family in a living room. The cartoon features a family with a young boy and a young girl, with the mother holding a small airplane. The children are positioned in different parts of the room, with one of them pointing to the airplane, while the other one is pointing to the woman.\n",
        "\n",
        "# Image 3 Caption: The image features two boys sitting at a table, creating paper airplanes. The boys are surrounded by paper, with one of them displaying a paper airplane in the middle of the table. The other boy is sitting on the side, with the paper airplane in the middle of the table. The scene is a depiction of a playful activity, with the boys enjoying their time together. The scene is a perfect example of how children can learn and develop their creativity through paper airplanes.\n",
        "\n",
        "# Image 4 Caption: The image features a cartoon scene of a family living in a home, with two children playing in the living room. The children are interacting with each other, flying paper planes and playing with toys. There are two children in the image, one in the middle and the other in the corner. The children are playing with paper planes, flying them around the room. There are two other children in the image, one in the middle and the other in the corner. The children are playing with toys and playing with paper planes, a common activity in many homes.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#InstructBLIP was also tried with an emotion aware prompy ( the only change made was in the folowing block)\n",
        "#https://colab.research.google.com/drive/1rnNW3gskrCwG8KTammaN06_1Ykrd-vTU?usp=sharing\n",
        "def generate_emotion_caption(image, prompt=\"Describe the emotions and mood shown in this image.\"):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=200,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        eos_token_id=processor.tokenizer.eos_token_id\n",
        "    )\n",
        "    caption = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "#captions:\n",
        "# Image 1 Caption:\n",
        "# The image depicts a cartoon of a child playing with a toy airplane. The children are shown in a playful mood, interacting with each other and playing with their toys. The cartoon is presented in a realistic setting, illustrating a family situation and a playful interaction between the children. The mood and emotions depicted in the image are characterized by playful energy, indicating a playful mood and a playful spirit.\n",
        "\n",
        "# Image 2 Caption:\n",
        "# The image shows a cartoon of a woman and her children who are crying over a plane accident. The woman is a mother, while the child is a brother, indicating that the family is experiencing a mishap. The image conveys a sense of sadness, anger, and confusion, with the woman's expression indicating that she is upset. The image could represent an accident that occurred in the home, which might have caused the mishap. The cartoon might also depict a serious problem, such as a broken home, which might have caused the mishap.\n",
        "\n",
        "# Image 3 Caption:\n",
        "# The image shows two boys in the room, playing around with paper airplanes and making them fly. The boy on the left side is smiling and the boy on the right side is smiling. There is a sense of enjoyment and fun, which can be seen in the image. It also shows the positive emotions and mood that can be experienced through play, as well as the importance of learning and sharing knowledge.\n",
        "\n",
        "# Image 4 Caption:\n",
        "# The cartoon shows a family in a playful mood, with the children playing with their paper airplanes in the living room. The children are likely engaged in a playful activity, demonstrating the joy and laughter that can be found in a family gathering. Besides the family, the image also shows a group of people, including a man and woman, who are in the kitchen, where they are preparing dinner and enjoying the food. The scene represents a happy, playful moment in the family, which demonstrates the bonding and closeness between the family members.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# model 6\n",
        "# Model name:LLaVA\n",
        "# Link:https://colab.research.google.com/drive/1rnNW3gskrCwG8KTammaN06_1Ykrd-vTU?usp=sharing\n",
        "# Code:\n",
        "!pip install transformers accelerate\n",
        "!pip install git+https://github.com/huggingface/peft.git\n",
        "!pip install bitsandbytes\n",
        "!pip install torch torchvision\n",
        "!pip install Pillow\n",
        "\n",
        "import torch\n",
        "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "model = LlavaForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    device_map=\"auto\",\n",
        "    load_in_8bit=True\n",
        ")\n",
        "\n",
        "def generate_llava_caption(image, prompt=\"Describe the image in detail.\"):\n",
        "    prompt = f\"<image>\\n{prompt}\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    inputs = processor(prompt, image, return_tensors=\"pt\").to(device, torch.float16)\n",
        "    output = model.generate(**inputs, max_new_tokens=200)\n",
        "    caption = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "image_urls = [\n",
        "    \"https://www.google.com/url?q=https%3A%2F%2Fi.ibb.co%2FRGH8hK6n%2FScreenshot-2025-06-26-145250.png\",\n",
        "    \"https://www.google.com/url?q=https%3A%2F%2Fi.ibb.co%2FMk4mjJhj%2FScreenshot-2025-06-26-145414.png\",\n",
        "    \"https://www.google.com/url?q=https%3A%2F%2Fi.ibb.co%2FjvVhChFp%2FScreenshot-2025-06-26-145429.png\",\n",
        "    \"https://www.google.com/url?q=https%3A%2F%2Fi.ibb.co%2F8gbYcM0d%2FScreenshot-2025-06-26-145506.png\"\n",
        "]\n",
        "\n",
        "import urllib.parse\n",
        "\n",
        "for idx, url in enumerate(image_urls):\n",
        "    # Extract the actual image URL from the Google redirect link\n",
        "    parsed_url = urllib.parse.urlparse(url)\n",
        "    actual_image_url = urllib.parse.parse_qs(parsed_url.query)['q'][0]\n",
        "\n",
        "    image = Image.open(requests.get(actual_image_url, stream=True).raw).convert(\"RGB\")\n",
        "    caption = generate_llava_caption(image)\n",
        "    print(f\"Image {idx+1} Caption:\\n{caption}\\n\")\n",
        "\n",
        "# Image 1 Caption:\n",
        "# Describe the image in detail.\n",
        "\n",
        "# The image depicts two young boys playing with a toy airplane in a living room. One boy is holding the airplane while the other boy is trying to take it from him. The room is furnished with a couch and a chair, and there is a bottle placed on a surface. The boys are enjoying their playtime, creating a lively atmosphere in the living room.\n",
        "\n",
        "# Image 2 Caption:\n",
        "# Describe the image in detail.\n",
        "\n",
        "# The image depicts a family scene with a woman and two children standing in a living room. The woman is wearing an apron, and the children are playing with a toy airplane. The woman is holding a spray bottle, possibly for cleaning or sanitizing purposes.\n",
        "\n",
        "# The living room is furnished with a couch and a chair, both placed against the wall. A dining table is located in the background, and a sink can be seen in the far right corner of the room. The family appears to be enjoying their time together in a comfortable and familiar setting.\n",
        "\n",
        "# Image 3 Caption:\n",
        "# Describe the image in detail.\n",
        "\n",
        "# The image features two children sitting at a dining table, engaged in a fun activity. They are cutting out paper airplanes, which are spread across the table. The children are focused on their task, and the table is filled with various colored paper airplanes.\n",
        "\n",
        "# In the background, there is a couch and a chair, providing a comfortable setting for the children to enjoy their activity. The scene is lively and filled with creativity, as the children work together to create their paper airplanes.\n",
        "\n",
        "# Image 4 Caption:\n",
        "# Describe the image in detail.\n",
        "\n",
        "# The image depicts two young boys playing with a kite in a living room. One boy is holding the kite, while the other is holding a remote control. The living room is furnished with a couch, a chair, and a dining table. There are also several bottles scattered around the room, possibly indicating a recent gathering or event. The boys seem to be enjoying their time together, engaging in a fun and interactive activity.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#LLaVA with an emotional prompt did not go that well\n",
        "#https://colab.research.google.com/drive/1NHMfqlNEe7LjMk8HNwfslfePEs9R3OBT?usp=sharing\n",
        "def generate_llava_emotion_caption(image, prompt=(\"Focus on the emotions, expressions, relationships, setting, and possible thoughts of the characters. \" \"Mention what is happening and what might happen next. \")):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    prompt = f\"<image>\\n{prompt}\"\n",
        "\n",
        "    # Correct input order\n",
        "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n",
        "\n",
        "    output = model.generate(**inputs, max_new_tokens=200)\n",
        "    caption = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "# Image 1 Emotion-Aware Caption:\n",
        "# Focus on the emotions, expressions, relationships, setting, and possible thoughts of the characters. Mention what is happening and what might happen next.\n",
        "# The two children are playing with a toy airplane in a living room. They are both holding the airplane and appear to be enjoying their playtime. The children are likely expressing excitement and joy as they engage in this imaginative activity. The setting is a cozy living room, which provides a comfortable environment for the children to play. The next possible event could be the children continuing their playtime, possibly taking turns flying the airplane or engaging in other imaginative scenarios.\n",
        "# Image 2 Emotion-Aware Caption:\n",
        "# Focus on the emotions, expressions, relationships, setting, and possible thoughts of the characters. Mention what is happening and what might happen next.\n",
        "# The image shows a woman standing in a kitchen, looking at two children who are crying. The woman appears to be concerned about the children's emotional state. The children are holding their hands to their faces, possibly indicating that they are upset or experiencing distress. The scene suggests that the woman is trying to comfort the children, possibly by offering them a hug or providing them with a soothing environment. The woman might be their mother or a caregiver, and she is likely trying to understand the cause of their distress and provide them with the necessary support.\n",
        "# Image 3 Emotion-Aware Caption:\n",
        "# Focus on the emotions, expressions, relationships, setting, and possible thoughts of the characters. Mention what is happening and what might happen next.\n",
        "# The two children are sitting at a table, engaged in a craft project. They are making paper kites, which are a popular and fun activity for children. The children are smiling and enjoying the process, which suggests that they are having a good time and are likely bonding over the activity. The next step in the project might involve cutting out the kites and assembling them, or they might be discussing their designs and sharing ideas. The scene depicts a positive and creative atmosphere, with the children learning and having fun together.\n",
        "# Image 4 Emotion-Aware Caption:\n",
        "# Focus on the emotions, expressions, relationships, setting, and possible thoughts of the characters. Mention what is happening and what might happen next.\n",
        "# The two boys are playing with a kite in a living room. One boy is holding the kite while the other boy is holding a remote control. They seem to be enjoying their time together, possibly learning how to fly the kite or just having fun. The living room has a couch, a chair, and a few bottles on a surface. The scene suggests a casual and relaxed atmosphere, with the boys engaging in a fun and interactiveÂ activity.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# model 7\n",
        "# Model name:CLIPcap+gpt2(failed)\n",
        "# Link:https://colab.research.google.com/drive/1NHMfqlNEe7LjMk8HNwfslfePEs9R3OBT?usp=sharing\n",
        "# Code:\n",
        "!pip install transformers ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install einops\n",
        "\n",
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "import requests\n",
        "import os\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load CLIP model (ViT-L/14)\n",
        "clip_model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
        "\n",
        "# Load GPT2 tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
        "\n",
        "def generate_clipcap_caption(image_path_or_url, prefix_text=\"The image shows\"):\n",
        "    # Load image from file or URL\n",
        "    if image_path_or_url.startswith(\"http\"):\n",
        "        image = Image.open(requests.get(image_path_or_url, stream=True).raw).convert(\"RGB\")\n",
        "    else:\n",
        "        image = Image.open(image_path_or_url).convert(\"RGB\")\n",
        "\n",
        "    # Preprocess image\n",
        "    image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "\n",
        "    # Encode image using CLIP\n",
        "    with torch.no_grad():\n",
        "        image_features = clip_model.encode_image(image_input).float()\n",
        "\n",
        "    # Prepare prompt\n",
        "    input_text = prefix_text.strip()\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate caption using GPT-2\n",
        "    outputs = gpt2.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_length=80,\n",
        "        num_return_sequences=1,\n",
        "        no_repeat_ngram_size=2,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        do_sample=True,\n",
        "        temperature=0.9,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "image_url = \"https://i.ibb.co/RGH8hK6/Screenshot-2025-06-26-145250.png\"\n",
        "\n",
        "caption = generate_clipcap_caption(\n",
        "    image_url,\n",
        "    prefix_text=\"Describe the emotional situation in the image. The image shows\"\n",
        ")\n",
        "\n",
        "print(\"Generated Emotion-Aware Caption:\\n\", caption)\n",
        "\n",
        "# The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
        "# Generated Emotion-Aware Caption:\n",
        "#  Describe the emotional situation in the image. The image shows the child's right arm.\n",
        "\n",
        "# A picture of the mother's left arm shows an open hand. There are two images of another child. Both images are identical and in fact both are of child abuse. It is only because of this that I have developed an understanding of a child as a person, as well as an ability to understand\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# model 8\n",
        "# Model name:kosmos2\n",
        "# Link:https://colab.research.google.com/drive/1KzFBRIKpGpOPO0YiD_P3NGvBDfbtVi3B?usp=sharing\n",
        "# Code:\n",
        "!pip install transformers==4.40.1 timm accelerate\n",
        "!pip install git+https://github.com/huggingface/peft.git\n",
        "\n",
        "import torch\n",
        "from transformers import AutoProcessor, Kosmos2ForConditionalGeneration\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load model and processor\n",
        "model_id = \"microsoft/kosmos-2-patch14-224\"\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "model = Kosmos2ForConditionalGeneration.from_pretrained(model_id).to(device)\n",
        "\n",
        "def generate_kosmos2_caption(image_url):\n",
        "    image = Image.open(requests.get(image_url, stream=True).raw).convert(\"RGB\")\n",
        "    prompt = \"<grounding> Describe the image in detail.\"\n",
        "\n",
        "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    generated_ids = model.generate(**inputs, max_new_tokens=50)\n",
        "    decoded = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    # Post-processing: Strip unnecessary garbage before <grounding> and after sentence\n",
        "    if \"<grounding>\" in decoded:\n",
        "        decoded = decoded.split(\"<grounding>\")[-1].strip()\n",
        "    return decoded\n",
        "\n",
        "image_urls = [\n",
        "    \"https://www.google.com/url?q=https%3A%2F%2Fi.ibb.co%2FRGH8hK6n%2FScreenshot-2025-06-26-145250.png\",\n",
        "    \"https://www.google.com/url?q=https%3A%2F%2Fi.ibb.co%2FMk4mjJhj%2FScreenshot-2025-06-26-145414.png\",\n",
        "    \"https://www.google.com/url?q=https%3A%2F%2Fi.ibb.co%2FjvVhChFp%2FScreenshot-2025-06-26-145429.png\",\n",
        "    \"https://www.google.com/url?q=https%3A%2F%2Fi.ibb.co%2F8gbYcM0d%2FScreenshot-2025-06-26-145506.png\"\n",
        "]\n",
        "\n",
        "import urllib.parse\n",
        "\n",
        "for idx, url in enumerate(image_urls):\n",
        "    # Extract the actual image URL from the Google redirect link\n",
        "    parsed_url = urllib.parse.urlparse(url)\n",
        "    actual_image_url = urllib.parse.parse_qs(parsed_url.query)['q'][0]\n",
        "\n",
        "    # image = Image.open(requests.get(actual_image_url, stream=True).raw).convert(\"RGB\")\n",
        "    caption = generate_kosmos2_caption(actual_image_url)\n",
        "    print(f\"Image {idx+1} Emotion-Aware Caption:\\n{caption}\\n\")\n",
        "\n",
        "# Image 1 Emotion-Aware Caption:\n",
        "# Describe the image in detail. The image depicts two young boys fighting over<phrase> a toy airplane</phrase><object><patch_index_0392><patch_index_0813></object>. One of the boys is holding the toy, while the other is holding a knife. The scene takes place in a living room, where<phrase> a woman\n",
        "\n",
        "# Image 2 Emotion-Aware Caption:\n",
        "# Describe the image in detail. The image depicts a family in a living room, with<phrase> a young boy</phrase><object><patch_index_0363><patch_index_0914></object> and<phrase> a girl</phrase><object><patch_index_0019><patch_index_0890></object> standing in the center of the room.<phrase> The boy</phrase>I</phrase><object><patch_index_0193><patch_index_0968></object> is crying,\n",
        "\n",
        "# Image 3 Emotion-Aware Caption:\n",
        "# Describe the image in detail. The image depicts two children sitting at a table, each holding<phrase> a piece of paper</phrase><object><patch_index_0614><patch_index_0877></object>. They appear to be engaged in a fun activity, as they are engaged in folding and cutting<phrase> colorful paper</phrase></delimiter_of_multi_objects/><patch_index_0801><patch_index_0999></delimiter_of_multi_objects/><patch_index_0624>\n",
        "\n",
        "# Image 4 Emotion-Aware Caption:\n",
        "# Describe the image in detail. The image depicts a living room with two children playing with<phrase> kites</phrase><object><patch_index_0435><patch_index_0632></delimiter_of_multi_objects/><patch_index_0361><patch_index_0555></object>. One child is holding a kite, while the other is holding another kite. The children are standing in a circle, with the kites\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cb9911c",
      "metadata": {
        "id": "7cb9911c"
      },
      "source": [
        "Update the google doc, and also store the captions generated by each model in a file, you can use any format you like, but I would suggest using CSV format, as it is easy to handle using pandas."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23653b8a",
      "metadata": {
        "id": "23653b8a"
      },
      "source": [
        "#### Story Generation\n",
        "Once you have generated the captions for each image, you can use a language model to generate a story using these captions.\n",
        "\n",
        "Again you have to experiment with different open-source Large Language Models(LLMs) and document the results in the google doc.\n",
        "\n",
        "LLMs are very powerful models for language generation tasks, these are based on transformer architecture that we studied in past sessions. And they have been trained on a large corpus of text data, so they are capable of handling a wide range of language tasks, including text generation, text classification, text summarization, etc.\n",
        "\n",
        "One of the proprietary LLMs that you use in your daily life is ChatGPT, you can also use that for this, but the problem is that it's API is not free, so for automating the process you will have to use open-source LLMs again from Hugging Face.\n",
        "\n",
        "As done in captioning, you have to try out atleast 6-7 different LLMs for story generation, some examples include:\n",
        "- Meta's Llama (including different versions, as well as various finetuned versions of it)\n",
        "- Mistral\n",
        "- Gemini API (This is free upto a certain limit, so can try it out)\n",
        "\n",
        "You can find many more here: [LLM leaderboard](https://huggingface.co/collections/open-llm-leaderboard/open-llm-leaderboard-best-models-652d6c7965a4619fb5c27a03) or [here](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b8c5f53",
      "metadata": {
        "id": "1b8c5f53"
      },
      "source": [
        "You can find details on how to use these models on their respective Hugging Face pages, or external documentations if any.\n",
        "\n",
        "The things that I told about quantization in image captioning also applies to LLMs, so you can use `bitsandbytes` library to load the models in quantized format if they are too large to run on your resources.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "402aa5e2",
      "metadata": {
        "id": "402aa5e2"
      },
      "source": [
        "One of the other important skill is Prompt Engineering, which is the art of crafting effective prompts to get the desired output from a language model.\n",
        "\n",
        "You won't get your desired output in the first go, so you will have to experiment with different prompts and see which one works best for your task, and also the model you are using.\n",
        "You can use the following prompt as a starting point:\n",
        "```python\n",
        "prompt = f\"\"\"\n",
        "You are a creative writer, and you have to write a story based on captions of a sequence of images.\n",
        "{captions}\n",
        "Write a detailed story that connects these captions in a meaningful way.\n",
        "\"\"\"\n",
        "```\n",
        "You can also instruct to include specific elements in the story, like characters, setting, etc. or to follow a particular style or tone.\n",
        "Note that this is a very basic prompt, tweak it so as to get the best output from the model you are using, experimentation is the key here."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b6b44e7",
      "metadata": {
        "id": "9b6b44e7"
      },
      "source": [
        "Very IMP: Make sure to store the results of captions, as well as story that you generated after loading these models in a file, so that you won't have to run the models again and again, you will eventually realise that loading  these models i svery tedious and time consuming, also you will have to face a lot of errors while doing so, so you will also learn how to debug these errors, which is a very important skill in itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe6be26c",
      "metadata": {
        "id": "fe6be26c"
      },
      "outputs": [],
      "source": [
        "# Code for Story Generation\n",
        "# Feel free to use Kaggle or Colab for actually running the code, but paste it here after you are done.\n",
        "\n",
        "# model 1\n",
        "# Model name: Gemma 7B Instruct\n",
        "# Link: https://colab.research.google.com/drive/18cU93eFJm9XdUbMm-0Kksm013z7Jnufv#scrollTo=zHdHnJ4jZw2h\n",
        "# Code:\n",
        "!pip install -q transformers accelerate bitsandbytes\n",
        "!pip install -q sentencepiece\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
        "import torch\n",
        "model_id = \"google/gemma-7b-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    load_in_4bit=True,\n",
        "    torch_dtype=\"auto\",\n",
        "    token=hf_token\n",
        ")\n",
        "\n",
        "streamer = TextStreamer(tokenizer)\n",
        "captions = [\n",
        "    \"The image depicts two young boys playing with a toy airplane in a living room. One boy is holding the airplane while the other boy is trying to take it from him. The room is furnished with a couch and a chair, and there is a bottle placed on a surface. The boys are enjoying their playtime, creating a lively atmosphere in the living room.\",\n",
        "    \"The image depicts a family scene with a woman and two children standing in a living room. The woman is wearing an apron, and the children are playing with a toy airplane. The woman is holding a spray bottle, possibly for cleaning or sanitizing purposes. The living room is furnished with a couch and a chair, both placed against the wall. A dining table is located in the background, and a sink can be seen in the far right corner of the room. The family appears to be enjoying their time together in a comfortable and familiar setting.\",\n",
        "    \"The image features two children sitting at a dining table, engaged in a fun activity. They are cutting out paper airplanes, which are spread across the table. The children are focused on their task, and the table is filled with various colored paper airplanes. In the background, there is a couch and a chair, providing a comfortable setting for the children to enjoy their activity. The scene is lively and filled with creativity, as the children work together to create their paper airplanes.\",\n",
        "    \"The image depicts two young boys playing with a kite in a living room. One boy is holding the kite, while the other is holding a remote control. The living room is furnished with a couch, a chair, and a dining table. There are also several bottles scattered around the room, possibly indicating a recent gathering or event. The boys seem to be enjoying their time together, engaging in a fun and interactive activity.\"\n",
        "]\n",
        "prompt = \"You are a master storyteller. Write a vivid, imaginative, and emotionally engaging story that is approximately 600 words long. Tie all events together into a single coherent plot with a satisfying ending. Begin the story now:\\n\"\n",
        "for i, caption in enumerate(captions, 1):\n",
        "    prompt += f\"{i}. {caption}\\n\"\n",
        "prompt += \"\\nStory:\\n\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=1000,\n",
        "        do_sample=True,\n",
        "        temperature=0.8,\n",
        "        top_p=0.9,\n",
        "        streamer=streamer\n",
        "    )\n",
        "# model 2\n",
        "# Model name: TheBloke/Mistral 7B Instruct v0.1\n",
        "# Link: https://colab.research.google.com/drive/1y7iLMF8mm3Sa88eEbhahlPZjfTfAh_rz#scrollTo=HIphASUJadh0\n",
        "# Code:\n",
        "!pip install -q transformers accelerate bitsandbytes auto-gptq\n",
        "from transformers import AutoTokenizer, TextStreamer\n",
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "import torch\n",
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "from transformers import AutoTokenizer, TextStreamer\n",
        "\n",
        "model_id = \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "\n",
        "model = AutoGPTQForCausalLM.from_quantized(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    use_safetensors=True,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    inject_fused_attention=False\n",
        ")\n",
        "\n",
        "streamer = TextStreamer(tokenizer)\n",
        "captions = [\n",
        "    \"The image depicts two young boys playing with a toy airplane in a living room. One boy is holding the airplane while the other boy is trying to take it from him. The room is furnished with a couch and a chair, and there is a bottle placed on a surface. The boys are enjoying their playtime, creating a lively atmosphere in the living room.\",\n",
        "    \"The image depicts a family scene with a woman and two children standing in a living room. The woman is wearing an apron, and the children are playing with a toy airplane. The woman is holding a spray bottle, possibly for cleaning or sanitizing purposes. The living room is furnished with a couch and a chair, both placed against the wall. A dining table is located in the background, and a sink can be seen in the far right corner of the room. The family appears to be enjoying their time together in a comfortable and familiar setting.\",\n",
        "    \"The image features two children sitting at a dining table, engaged in a fun activity. They are cutting out paper airplanes, which are spread across the table. The children are focused on their task, and the table is filled with various colored paper airplanes. In the background, there is a couch and a chair, providing a comfortable setting for the children to enjoy their activity. The scene is lively and filled with creativity, as the children work together to create their paper airplanes.\",\n",
        "    \"The image depicts two young boys playing with a kite in a living room. One boy is holding the kite, while the other is holding a remote control. The living room is furnished with a couch, a chair, and a dining table. There are also several bottles scattered around the room, possibly indicating a recent gathering or event. The boys seem to be enjoying their time together, engaging in a fun and interactive activity.\"\n",
        "]\n",
        "\n",
        "prompt = \"You are a master storyteller. Write a vivid, imaginative, and emotionally engaging story that is approximately 800 words long. Tie all events together into a single coherent plot with a satisfying ending. Begin the story now:\\n\"\n",
        "prompt += \"\\n\".join([f\"{i+1}. {caption}\" for i, caption in enumerate(captions)])\n",
        "prompt += \"\\n\\nStory:\\n\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=1300,\n",
        "    temperature=0.85,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.1,\n",
        "    do_sample=True,\n",
        "    streamer=streamer\n",
        ")\n",
        "\n",
        "# model 3\n",
        "# Model name: Gemini API\n",
        "# Link:\n",
        "# Code:\n",
        "!pip install -q google-generativeai\n",
        "import google.generativeai as genai\n",
        "genai.configure(api_key=\"enter_your_api\")\n",
        "captions = [\n",
        "    \"The image depicts two young boys playing with a toy airplane in a living room. One boy is holding the airplane while the other boy is trying to take it from him. The room is furnished with a couch and a chair, and there is a bottle placed on a surface. The boys are enjoying their playtime, creating a lively atmosphere in the living room.\",\n",
        "    \"The image depicts a family scene with a woman and two children standing in a living room. The woman is wearing an apron, and the children are playing with a toy airplane. The woman is holding a spray bottle, possibly for cleaning or sanitizing purposes. The living room is furnished with a couch and a chair, both placed against the wall. A dining table is located in the background, and a sink can be seen in the far right corner of the room. The family appears to be enjoying their time together in a comfortable and familiar setting.\",\n",
        "    \"The image features two children sitting at a dining table, engaged in a fun activity. They are cutting out paper airplanes, which are spread across the table. The children are focused on their task, and the table is filled with various colored paper airplanes. In the background, there is a couch and a chair, providing a comfortable setting for the children to enjoy their activity. The scene is lively and filled with creativity, as the children work together to create their paper airplanes.\",\n",
        "    \"The image depicts two young boys playing with a kite in a living room. One boy is holding the kite, while the other is holding a remote control. The living room is furnished with a couch, a chair, and a dining table. There are also several bottles scattered around the room, possibly indicating a recent gathering or event. The boys seem to be enjoying their time together, engaging in a fun and interactive activity.\"\n",
        "]\n",
        "prompt = \"You are a master storyteller. Write a vivid, imaginative, and emotionally engaging story that is approximately 800 words long. Tie all events together into a single coherent plot with a satisfying ending. Begin the story now:\\n\"\n",
        "prompt += \"\\n\".join([f\"{i+1}. {caption}\" for i, caption in enumerate(captions)])\n",
        "prompt += \"\\n\\nStory:\\n\"\n",
        "model = genai.GenerativeModel(model_name=\"models/gemini-1.5-flash-latest\")\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)\n",
        "\n",
        "# model 4\n",
        "# Model name: Llama-2-7B-Chat-GPTQ\n",
        "# Link: https://colab.research.google.com/drive/1iXnMYL8q3bC06GTdB7MvODfEy0pMDitF#scrollTo=BH9gydDdh5fL\n",
        "# Code:\n",
        "!pip install -q transformers accelerate bitsandbytes auto-gptq optimum\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
        "import torch\n",
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "model_id = \"TheBloke/Llama-2-7B-Chat-GPTQ\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "\n",
        "model = AutoGPTQForCausalLM.from_quantized(\n",
        "    model_id,\n",
        "    device_map=\"auto\",  # use GPU automatically\n",
        "    use_safetensors=True,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "streamer = TextStreamer(tokenizer)\n",
        "\n",
        "# Add your captions\n",
        "captions = [\n",
        "    \"The image depicts two young boys playing with a toy airplane in a living room. One boy is holding the airplane while the other boy is trying to take it from him. The room is furnished with a couch and a chair, and there is a bottle placed on a surface. The boys are enjoying their playtime, creating a lively atmosphere in the living room.\",\n",
        "    \"The image depicts a family scene with a woman and two children standing in a living room. The woman is wearing an apron, and the children are playing with a toy airplane. The woman is holding a spray bottle, possibly for cleaning or sanitizing purposes. The living room is furnished with a couch and a chair, both placed against the wall. A dining table is located in the background, and a sink can be seen in the far right corner of the room. The family appears to be enjoying their time together in a comfortable and familiar setting.\",\n",
        "    \"The image features two children sitting at a dining table, engaged in a fun activity. They are cutting out paper airplanes, which are spread across the table. The children are focused on their task, and the table is filled with various colored paper airplanes. In the background, there is a couch and a chair, providing a comfortable setting for the children to enjoy their activity. The scene is lively and filled with creativity, as the children work together to create their paper airplanes.\",\n",
        "    \"The image depicts two young boys playing with a kite in a living room. One boy is holding the kite, while the other is holding a remote control. The living room is furnished with a couch, a chair, and a dining table. There are also several bottles scattered around the room, possibly indicating a recent gathering or event. The boys seem to be enjoying their time together, engaging in a fun and interactive activity.\"\n",
        "]\n",
        "prompt = \"You are a master storyteller. Write a vivid, imaginative, and emotionally engaging story that is approximately 600 words long. Tie all events together into a single coherent plot with a satisfying ending. Begin the story now:\\n\"\n",
        "prompt += \"\\n\".join([f\"{i+1}. {cap}\" for i, cap in enumerate(captions)])\n",
        "prompt += \"\\n\\nStory:\\n\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=1000,\n",
        "        do_sample=True,\n",
        "        temperature=0.8,\n",
        "        top_p=0.95,\n",
        "        streamer=streamer\n",
        "    )\n",
        "\n",
        "# model 5\n",
        "# Model name: Mistral 7B Instruct v0.1\n",
        "# Link: https://colab.research.google.com/drive/12xpaXy5g75QQhs1xF3mtSRT_ZMuBBK9v#scrollTo=nesG2NpaKf2j\n",
        "# Code:\n",
        "!pip install -q transformers accelerate bitsandbytes\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "login()\n",
        "\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    load_in_4bit=True,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "streamer = TextStreamer(tokenizer)\n",
        "#caption\n",
        "captions = [\n",
        "    \"The image depicts two young boys playing with a toy airplane in a living room. One boy is holding the airplane while the other boy is trying to take it from him. The room is furnished with a couch and a chair, and there is a bottle placed on a surface. The boys are enjoying their playtime, creating a lively atmosphere in the living room.\",\n",
        "    \"The image depicts a family scene with a woman and two children standing in a living room. The woman is wearing an apron, and the children are playing with a toy airplane. The woman is holding a spray bottle, possibly for cleaning or sanitizing purposes. The living room is furnished with a couch and a chair, both placed against the wall. A dining table is located in the background, and a sink can be seen in the far right corner of the room. The family appears to be enjoying their time together in a comfortable and familiar setting.\",\n",
        "    \"The image features two children sitting at a dining table, engaged in a fun activity. They are cutting out paper airplanes, which are spread across the table. The children are focused on their task, and the table is filled with various colored paper airplanes. In the background, there is a couch and a chair, providing a comfortable setting for the children to enjoy their activity. The scene is lively and filled with creativity, as the children work together to create their paper airplanes.\",\n",
        "    \"The image depicts two young boys playing with a kite in a living room. One boy is holding the kite, while the other is holding a remote control. The living room is furnished with a couch, a chair, and a dining table. There are also several bottles scattered around the room, possibly indicating a recent gathering or event. The boys seem to be enjoying their time together, engaging in a fun and interactive activity.\"\n",
        "]\n",
        "prompt = \"You are a master storyteller. Write a vivid, imaginative, and emotionally engaging story that is approximately 800 words long. Tie all events together into a single coherent plot with a satisfying ending. Begin the story now:\\n\"\n",
        "prompt += \"\\n\".join([f\"{i+1}. {caption}\" for i, caption in enumerate(captions)])\n",
        "prompt += \"\\n\\nStory:\\n\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=1300,\n",
        "    do_sample=True,\n",
        "    temperature=0.8,\n",
        "    top_p=0.9,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "story = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"\\n\\nGenerated Story:\\n\", story)\n",
        "# model 6\n",
        "# Model name: MythoMax-L2 13B GPTQ\n",
        "# Link: https://colab.research.google.com/drive/1hv5DVl5Q1oqVY8RP_CrZ9uKDGc6JDtyI?authuser=1#scrollTo=mApH9OFAkwRI\n",
        "# Code:\n",
        "!pip install -q transformers accelerate bitsandbytes auto-gptq\n",
        "import torch\n",
        "from transformers import AutoTokenizer, TextStreamer\n",
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "model_id = \"TheBloke/MythoMax-L2-13B-GPTQ\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "\n",
        "model = AutoGPTQForCausalLM.from_quantized(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    use_safetensors=True,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    inject_fused_attention=False\n",
        ")\n",
        "streamer = TextStreamer(tokenizer)\n",
        "\n",
        "# caption\n",
        "captions = [\n",
        "    \"The image depicts two young boys playing with a toy airplane in a living room. One boy is holding the airplane while the other boy is trying to take it from him. The room is furnished with a couch and a chair, and there is a bottle placed on a surface. The boys are enjoying their playtime, creating a lively atmosphere in the living room.\",\n",
        "    \"The image depicts a family scene with a woman and two children standing in a living room. The woman is wearing an apron, and the children are playing with a toy airplane. The woman is holding a spray bottle, possibly for cleaning or sanitizing purposes. The living room is furnished with a couch and a chair, both placed against the wall. A dining table is located in the background, and a sink can be seen in the far right corner of the room. The family appears to be enjoying their time together in a comfortable and familiar setting.\",\n",
        "    \"The image features two children sitting at a dining table, engaged in a fun activity. They are cutting out paper airplanes, which are spread across the table. The children are focused on their task, and the table is filled with various colored paper airplanes. In the background, there is a couch and a chair, providing a comfortable setting for the children to enjoy their activity. The scene is lively and filled with creativity, as the children work together to create their paper airplanes.\",\n",
        "    \"The image depicts two young boys playing with a kite in a living room. One boy is holding the kite, while the other is holding a remote control. The living room is furnished with a couch, a chair, and a dining table. There are also several bottles scattered around the room, possibly indicating a recent gathering or event. The boys seem to be enjoying their time together, engaging in a fun and interactive activity.\"\n",
        "]\n",
        "\n",
        "#Create a prompt\n",
        "prompt = \"You are a master storyteller. Write a vivid, imaginative, and emotionally engaging story that is approximately 800 words long. Tie all events together into a single coherent plot with a satisfying ending. Begin the story now:\\n\"\n",
        "prompt += \"\\n\".join([f\"{i+1}. {caption}\" for i, caption in enumerate(captions)])\n",
        "prompt += \"\\n\\nStory:\\n\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "#Generate story\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=1300,\n",
        "        temperature=0.85,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.1,\n",
        "        do_sample=True,\n",
        "        streamer=streamer\n",
        "    )\n",
        "\n",
        "# model 7\n",
        "# Model name: OpenHermes 2.5 Mistral 7B\n",
        "# Link: https://colab.research.google.com/drive/1B8phoMJ_tSg2UrbTqT-64flbz1Rw3IJB?authuser=2#scrollTo=xLPj0PBOlk2F\n",
        "# Code:\n",
        "!pip install -q transformers accelerate bitsandbytes auto-gptq\n",
        "import torch\n",
        "from transformers import AutoTokenizer, TextStreamer\n",
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "from huggingface_hub import login\n",
        "login()\n",
        "model_id = \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "\n",
        "model = AutoGPTQForCausalLM.from_quantized(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    use_safetensors=True,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "streamer = TextStreamer(tokenizer)\n",
        "captions = [\n",
        "    \"The image depicts two young boys playing with a toy airplane in a living room. One boy is holding the airplane while the other boy is trying to take it from him. The room is furnished with a couch and a chair, and there is a bottle placed on a surface. The boys are enjoying their playtime, creating a lively atmosphere in the living room.\",\n",
        "    \"The image depicts a family scene with a woman and two children standing in a living room. The woman is wearing an apron, and the children are playing with a toy airplane. The woman is holding a spray bottle, possibly for cleaning or sanitizing purposes. The living room is furnished with a couch and a chair, both placed against the wall. A dining table is located in the background, and a sink can be seen in the far right corner of the room. The family appears to be enjoying their time together in a comfortable and familiar setting.\",\n",
        "    \"The image features two children sitting at a dining table, engaged in a fun activity. They are cutting out paper airplanes, which are spread across the table. The children are focused on their task, and the table is filled with various colored paper airplanes. In the background, there is a couch and a chair, providing a comfortable setting for the children to enjoy their activity. The scene is lively and filled with creativity, as the children work together to create their paper airplanes.\",\n",
        "    \"The image depicts two young boys playing with a kite in a living room. One boy is holding the kite, while the other is holding a remote control. The living room is furnished with a couch, a chair, and a dining table. There are also several bottles scattered around the room, possibly indicating a recent gathering or event. The boys seem to be enjoying their time together, engaging in a fun and interactive activity.\"\n",
        "]\n",
        "prompt = \"You are a master storyteller. Write a vivid, imaginative, and emotionally engaging story that is approximately 800 words long. Tie all events together into a single coherent plot with a satisfying ending. Begin the story now:\\n\"\n",
        "prompt += \"\\n\".join([f\"{i+1}. {cap}\" for i, cap in enumerate(captions)])\n",
        "prompt += \"\\n\\nStory:\\n\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "with torch.no_grad():\n",
        "    with torch.autocast(\"cuda\", enabled=False):\n",
        "        output = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=1300,\n",
        "            temperature=0.85,\n",
        "            top_p=0.95,\n",
        "            repetition_penalty=1.1,\n",
        "            do_sample=True,\n",
        "            streamer=streamer\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "422ac20e",
      "metadata": {
        "id": "422ac20e"
      },
      "source": [
        "The End! ð«¡\n",
        "Hope you enjoyed this assignment, and learned something new."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}