{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8ce7c02",
   "metadata": {},
   "source": [
    "## Assignment 5\n",
    "\n",
    "This assignment is designed to help you understand, implement, and get hold of two important models in Multi-Modal Learning: CLIP and BLIP.\n",
    "\n",
    "Before starting to code, read the instructions and comments carefully. Make sure you understand the requirements and the expected output.\n",
    "\n",
    "There are two parts of this assignment.\n",
    "- Content based image retrieval (CBIR) based on CLIP\n",
    "- Visual Question Answering (VQA) based on BLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bed11c",
   "metadata": {},
   "source": [
    "## Part 1: Content Based Image Retrieval (CBIR)\n",
    "\n",
    "Ever found yourself looking for a specific image in a load of photos on your phone?\n",
    "Imagine you have a system that can retrieve images based on their content. In this assignment, you will implement exactly that.\n",
    "\n",
    "### Task\n",
    "1. I have provided a list of images in form of unsplash links. Feel free to explore and add more images.\n",
    "2. You will have to implement CBIR system that will retrive images based on the query you provide.\n",
    "3. You have to use CLIP model to extract image and text embeddings. Since CLIP projects both image and text into the same embedding space, similar images and their corresponding text descriptions will be close to each other in the embedding space.\n",
    "4. You have to use a pretrained CLIP model from HuggingFace. (Because it is not feasible to train CLIP from scratch)\n",
    "5. You have to use [OpenAI's CLIP](https://huggingface.co/openai/clip-vit-base-patch32) model from HuggingFace. Refer to the page itself and anyother resources you find useful, on how to use it.\n",
    "6. You will have to take an input query from user, and display the best image matching the query. (You can use cosine similarity to find similarity between the query and the images, then display the image with highest similarity score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97e699c",
   "metadata": {},
   "source": [
    "##### Load all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f34700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load libraries\n",
    "\n",
    "IMAGE_URLS = [\n",
    "    \"https://images.unsplash.com/photo-1750126833705-ba98013f16f3\",\n",
    "    \"https://images.unsplash.com/photo-1749627995669-4d4dda3a9c1d\",\n",
    "    \"https://images.unsplash.com/photo-1744294724362-3f5c404c771a\",\n",
    "    \"https://images.unsplash.com/photo-1750075750236-3f8924fc0e35\"\n",
    "]\n",
    "# Feel free to edit the above list with your own image URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56ca234",
   "metadata": {},
   "source": [
    "##### Load the CLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a780876d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b038eedb",
   "metadata": {},
   "source": [
    "##### Loop through images, convert then to tensors, and store them in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a973d78d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bfe3774",
   "metadata": {},
   "source": [
    "##### Get the embeddings for the images and store them in a list\n",
    "Use the CLIP model linked above, the page also provides details on how to use it. If you have any issues with it, feel free to use anyother resources you find useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dbc778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70aa562f",
   "metadata": {},
   "source": [
    "##### Get the query from user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57af58b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input(\"Enter which image are you looking for: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90652456",
   "metadata": {},
   "source": [
    "##### Get the embeddings for the user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35de42a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81297b77",
   "metadata": {},
   "source": [
    "##### Find the similarity between the query and all the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f860b97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09b79ae2",
   "metadata": {},
   "source": [
    "##### Display the image with highest similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249ded07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65699817",
   "metadata": {},
   "source": [
    "Congrats! You have implemented a simple Content Based Image Retrieval (CBIR) system using CLIP model.\n",
    "\n",
    "Moving on to the next part of the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34447ff0",
   "metadata": {},
   "source": [
    "## Part 2: Visual Question Answering (VQA)\n",
    "\n",
    "This part is based on Visual Question Answering (VQA), where you will be given an image and a question about that image, and you will need to provide an answer.\n",
    "\n",
    "You are provided with a dataset of 10 images, and a json file containing questions and answers for those images.\n",
    "\n",
    "### Format of JSON file\n",
    "```json\n",
    "[\n",
    "    \"what is the blue shape?\",\n",
    "    \"rectangle\",\n",
    "    0\n",
    "]\n",
    "```\n",
    "This means that the question is \"what is the blue shape?\", the answer is \"rectangle\", and the image index is 0.\n",
    "Images are named from 0 to 9, so the image for index 0 is `0.png`.\n",
    "\n",
    "### Task\n",
    "1. Load the JSON file and parse the questions and answers.\n",
    "2. Just for understanding, print a few questions and their corresponding answers, also display the corresponding images. (You may do it for 1 or 2 questions)\n",
    "3. Implement a function that takes an image and a question, and returns the answer.\n",
    "4. Run the function on all questions and store the answers in a file. (You can store it in a new JSON or CSV file) (May use pandas for this)\n",
    "5. Now since you have the answers, you can also evaluate your model. For this, you can compare your answers with the ground truth answers provided in the JSON file. (refer to note below)\n",
    "\n",
    "Note: But it is not that easy to compare directly, as the answers may not match exactly due to variations in phrasing (\"2\" and \"two\", \"blue\" and \"The color is blue\". Phrases like this don't match, but are correct). So you should check your answers manually and see if they are correct or not. Do it for first 30 answers, and calculate the accuracy.\n",
    "There are certain ways to automate this, but they will make this assignment too complicated, so we will not do that here.\n",
    "\n",
    "### Additional Points\n",
    "- Refer to BLIP model that was taught in last class. It can be used to answer questions based on images.\n",
    "- You will have to use pre-trained BLIP model for this task. (Because training it from scratch is not feasible in this assignment)\n",
    "- You have to use [Salesforce BLIP model](https://huggingface.co/Salesforce/blip-vqa-base) for this task. Refer to the Usage section of page for details on how to use it. If you feel stuck, you can refer to blogs available online, or use AI tools to get yourself familiar with the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6c4c1a",
   "metadata": {},
   "source": [
    "1. Load libraries, and the JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f8fcaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a47dacf",
   "metadata": {},
   "source": [
    "2. Display a few questions and answers along with images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce794936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d89517c2",
   "metadata": {},
   "source": [
    "3. Implement the function to answer questions based on images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdf8ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vqa(question, image):\n",
    "    # write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61682d6",
   "metadata": {},
   "source": [
    "4. Run the function on all questions and store the answers in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf25aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4579dc17",
   "metadata": {},
   "source": [
    "5. Evaluate the model by comparing answers with ground truth.\n",
    "Do the comparison manually for the first 30 answers and print the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe80d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75a5cb6e",
   "metadata": {},
   "source": [
    "Bonus: This images were too simple. But guess what? Salesforce has pretrained it's model on wide variety of images, so you can use it on any image of your choice.\n",
    "\n",
    "Try to load any real world image of any natural scene, or anything you like, and ask questions about it. You will be surprised by the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32cb81c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b83d31b",
   "metadata": {},
   "source": [
    "The End 🫡.\n",
    "\n",
    "If you have any questions, feel free to use following in the given order:\n",
    "- Google it!!\n",
    "- ChatGPT\n",
    "- Ask on the group"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
