{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8ce7c02",
   "metadata": {},
   "source": [
    "## Assignment 5\n",
    "\n",
    "This assignment is designed to help you understand, implement, and get hold of two important models in Multi-Modal Learning: CLIP and BLIP.\n",
    "\n",
    "Before starting to code, read the instructions and comments carefully. Make sure you understand the requirements and the expected output.\n",
    "\n",
    "There are two parts of this assignment.\n",
    "- Content based image retrieval (CBIR) based on CLIP\n",
    "- Visual Question Answering (VQA) based on BLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bed11c",
   "metadata": {},
   "source": [
    "## Part 1: Content Based Image Retrieval (CBIR)\n",
    "\n",
    "Ever found yourself looking for a specific image in a load of photos on your phone?\n",
    "Imagine you have a system that can retrieve images based on their content. In this assignment, you will implement exactly that.\n",
    "\n",
    "### Task\n",
    "1. I have provided a list of images in form of unsplash links. Feel free to explore and add more images.\n",
    "2. You will have to implement CBIR system that will retrive images based on the query you provide.\n",
    "3. You have to use CLIP model to extract image and text embeddings. Since CLIP projects both image and text into the same embedding space, similar images and their corresponding text descriptions will be close to each other in the embedding space.\n",
    "4. You have to use a pretrained CLIP model from HuggingFace. (Because it is not feasible to train CLIP from scratch)\n",
    "5. You have to use [OpenAI's CLIP](https://huggingface.co/openai/clip-vit-base-patch32) model from HuggingFace. Refer to the page itself and anyother resources you find useful, on how to use it.\n",
    "6. You will have to take an input query from user, and display the best image matching the query. (You can use cosine similarity to find similarity between the query and the images, then display the image with highest similarity score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97e699c",
   "metadata": {},
   "source": [
    "##### Load all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f34700f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (4.52.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: torchvision in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (0.22.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.7.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torchvision) (2.7.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch==2.7.1->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch==2.7.1->torchvision) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch==2.7.1->torchvision) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch==2.7.1->torchvision) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch==2.7.1->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch==2.7.1->torchvision) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch==2.7.1->torchvision) (69.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch==2.7.1->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from jinja2->torch==2.7.1->torchvision) (2.1.3)\n",
      "Requirement already satisfied: pillow in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (10.3.0)\n"
     ]
    }
   ],
   "source": [
    "## Load libraries\n",
    "!pip install transformers\n",
    "!pip install torchvision\n",
    "!pip install pillow\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "IMAGE_URLS = [\n",
    "    \"https://images.unsplash.com/photo-1750126833705-ba98013f16f3\",\n",
    "    \"https://images.unsplash.com/photo-1749627995669-4d4dda3a9c1d\",\n",
    "    \"https://images.unsplash.com/photo-1744294724362-3f5c404c771a\",\n",
    "    \"https://images.unsplash.com/photo-1750075750236-3f8924fc0e35\"\n",
    "]\n",
    "# Feel free to edit the above list with your own image URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56ca234",
   "metadata": {},
   "source": [
    "##### Load the CLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a780876d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b038eedb",
   "metadata": {},
   "source": [
    "##### Loop through images, convert then to tensors, and store them in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a973d78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),   \n",
    "    transforms.ToTensor(),           \n",
    "    transforms.Normalize(            \n",
    "        mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "        std=[0.26862954, 0.26130258, 0.27577711]\n",
    "    )\n",
    "])\n",
    "image_tensors = []\n",
    "images = []\n",
    "for url in IMAGE_URLS:\n",
    "    response = requests.get(url, stream=True)\n",
    "    img = Image.open(response.raw).convert(\"RGB\")\n",
    "    images.append(img)                            \n",
    "    tensor = transform(img)                       \n",
    "    image_tensors.append(tensor)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfe3774",
   "metadata": {},
   "source": [
    "##### Get the embeddings for the images and store them in a list\n",
    "Use the CLIP model linked above, the page also provides details on how to use it. If you have any issues with it, feel free to use anyother resources you find useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69dbc778",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "image_embeddings = []\n",
    "with torch.no_grad():\n",
    "    for img_tensor in image_tensors:\n",
    "        img_tensor = img_tensor.unsqueeze(0)  \n",
    "        inputs = {\"pixel_values\": img_tensor}\n",
    "        outputs = model.get_image_features(**inputs)\n",
    "        outputs = outputs / outputs.norm(p=2, dim=-1, keepdim=True)  \n",
    "        image_embeddings.append(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aa562f",
   "metadata": {},
   "source": [
    "##### Get the query from user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57af58b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input(\"Enter which image are you looking for: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90652456",
   "metadata": {},
   "source": [
    "##### Get the embeddings for the user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35de42a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(text=[query], return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    text_embedding = model.get_text_features(**inputs)\n",
    "    text_embedding = text_embedding / text_embedding.norm(p=2, dim=-1, keepdim=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81297b77",
   "metadata": {},
   "source": [
    "##### Find the similarity between the query and all the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f860b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "image_embeddings_tensor = torch.cat(image_embeddings, dim=0)  \n",
    "similarities = F.cosine_similarity(text_embedding, image_embeddings_tensor)\n",
    "best_match_index = similarities.argmax().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b79ae2",
   "metadata": {},
   "source": [
    "##### Display the image with highest similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "249ded07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most similar image index: 0\n",
      "Similarity score: 0.2944\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nMost similar image index: {best_match_index}\")\n",
    "print(f\"Similarity score: {similarities[best_match_index]:.4f}\")\n",
    "images[best_match_index].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65699817",
   "metadata": {},
   "source": [
    "Congrats! You have implemented a simple Content Based Image Retrieval (CBIR) system using CLIP model.\n",
    "\n",
    "Moving on to the next part of the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34447ff0",
   "metadata": {},
   "source": [
    "## Part 2: Visual Question Answering (VQA)\n",
    "\n",
    "This part is based on Visual Question Answering (VQA), where you will be given an image and a question about that image, and you will need to provide an answer.\n",
    "\n",
    "You are provided with a dataset of 10 images, and a json file containing questions and answers for those images.\n",
    "\n",
    "### Format of JSON file\n",
    "```json\n",
    "[\n",
    "    \"what is the blue shape?\",\n",
    "    \"rectangle\",\n",
    "    0\n",
    "]\n",
    "```\n",
    "This means that the question is \"what is the blue shape?\", the answer is \"rectangle\", and the image index is 0.\n",
    "Images are named from 0 to 9, so the image for index 0 is `0.png`.\n",
    "\n",
    "### Task\n",
    "1. Load the JSON file and parse the questions and answers.\n",
    "2. Just for understanding, print a few questions and their corresponding answers, also display the corresponding images. (You may do it for 1 or 2 questions)\n",
    "3. Implement a function that takes an image and a question, and returns the answer.\n",
    "4. Run the function on all questions and store the answers in a file. (You can store it in a new JSON or CSV file) (May use pandas for this)\n",
    "5. Now since you have the answers, you can also evaluate your model. For this, you can compare your answers with the ground truth answers provided in the JSON file. (refer to note below)\n",
    "\n",
    "Note: But it is not that easy to compare directly, as the answers may not match exactly due to variations in phrasing (\"2\" and \"two\", \"blue\" and \"The color is blue\". Phrases like this don't match, but are correct). So you should check your answers manually and see if they are correct or not. Do it for first 30 answers, and calculate the accuracy.\n",
    "There are certain ways to automate this, but they will make this assignment too complicated, so we will not do that here.\n",
    "\n",
    "### Additional Points\n",
    "- Refer to BLIP model that was taught in last class. It can be used to answer questions based on images.\n",
    "- You will have to use pre-trained BLIP model for this task. (Because training it from scratch is not feasible in this assignment)\n",
    "- You have to use [Salesforce BLIP model](https://huggingface.co/Salesforce/blip-vqa-base) for this task. Refer to the Usage section of page for details on how to use it. If you feel stuck, you can refer to blogs available online, or use AI tools to get yourself familiar with the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6c4c1a",
   "metadata": {},
   "source": [
    "1. Load libraries, and the JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82454dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (4.52.4)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: torchvision in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (0.22.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: torch==2.7.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torchvision) (2.7.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch==2.7.1->torchvision) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch==2.7.1->torchvision) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch==2.7.1->torchvision) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch==2.7.1->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch==2.7.1->torchvision) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch==2.7.1->torchvision) (69.5.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch==2.7.1->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from jinja2->torch==2.7.1->torchvision) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59f8fcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9050550578cb4ced85f09a7b58e8556c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/445 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\LENOVO\\.cache\\huggingface\\hub\\models--Salesforce--blip-vqa-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84fff2155fa45a39ba66639c61567eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d7b83115ea4a9f93a4537aa4656056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5367f3b7a3a4d1bb4184d5a71732d71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8fe42dcc6b34809b287aa568600e3ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34f5a11416184d14a72aa303277ed2ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1566f8ab09be42559cd650d4c6a8e45c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
    "from PIL import Image\n",
    "import torch\n",
    "import json\n",
    "with open('filtered_questions.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a47dacf",
   "metadata": {},
   "source": [
    "2. Display a few questions and answers along with images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce794936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1\n",
      "Image: 0.png\n",
      "Q: what is the blue shape?\n",
      "A: rectangle\n",
      "------------------------------\n",
      "Example 2\n",
      "Image: 0.png\n",
      "Q: what color is the shape?\n",
      "A: blue\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(data):\n",
    "    question, answer, image_index = item\n",
    "    image_filename = f\"{image_index}.png\"\n",
    "\n",
    "\n",
    "    img = Image.open(image_filename)\n",
    "    img.show()\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(f\"Image: {image_filename}\")\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {answer}\")\n",
    "    print('-' * 30)\n",
    "    if i == 1:  \n",
    "        break \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89517c2",
   "metadata": {},
   "source": [
    "3. Implement the function to answer questions based on images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffdf8ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vqa(question, image):\n",
    "    # write code here\n",
    "    inputs = processor(image, question, return_tensors=\"pt\")\n",
    "    output = model.generate(**inputs)\n",
    "    answer = processor.decode(output[0], skip_special_tokens=True)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61682d6",
   "metadata": {},
   "source": [
    "4. Run the function on all questions and store the answers in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdf25aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results = []\n",
    "for item in data:\n",
    "    question, ground_truth, image_index = item\n",
    "    image_path = f\"{image_index}.png\"\n",
    "\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        predicted_answer = vqa(question, image)\n",
    "\n",
    "        results.append({\n",
    "            \"image\": image_path,\n",
    "            \"question\": question,\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"predicted_answer\": predicted_answer\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error with image {image_path}: {e}\")\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"vqa_predictions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4579dc17",
   "metadata": {},
   "source": [
    "5. Evaluate the model by comparing answers with ground truth.\n",
    "Do the comparison manually for the first 30 answers and print the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efe80d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 30 Predictions:\n",
      "\n",
      "1. Q: what is the blue shape?\n",
      "   Ground Truth:      rectangle\n",
      "   Predicted Answer:  rectangle\n",
      "--------------------------------------------------\n",
      "2. Q: what color is the shape?\n",
      "   Ground Truth:      blue\n",
      "   Predicted Answer:  black\n",
      "--------------------------------------------------\n",
      "3. Q: does the image contain a rectangle?\n",
      "   Ground Truth:      yes\n",
      "   Predicted Answer:  yes\n",
      "--------------------------------------------------\n",
      "4. Q: is there a triangle in the image?\n",
      "   Ground Truth:      no\n",
      "   Predicted Answer:  no\n",
      "--------------------------------------------------\n",
      "5. Q: is there a black shape?\n",
      "   Ground Truth:      no\n",
      "   Predicted Answer:  yes\n",
      "--------------------------------------------------\n",
      "6. Q: does the image not contain a gray shape?\n",
      "   Ground Truth:      yes\n",
      "   Predicted Answer:  yes\n",
      "--------------------------------------------------\n",
      "7. Q: is there a red shape in the image?\n",
      "   Ground Truth:      no\n",
      "   Predicted Answer:  no\n",
      "--------------------------------------------------\n",
      "8. Q: does the image not contain a red shape?\n",
      "   Ground Truth:      yes\n",
      "   Predicted Answer:  yes\n",
      "--------------------------------------------------\n",
      "9. Q: is there not a blue shape?\n",
      "   Ground Truth:      no\n",
      "   Predicted Answer:  yes\n",
      "--------------------------------------------------\n",
      "10. Q: is there not a blue shape in the image?\n",
      "   Ground Truth:      no\n",
      "   Predicted Answer:  yes\n",
      "--------------------------------------------------\n",
      "11. Q: is there not a yellow shape?\n",
      "   Ground Truth:      yes\n",
      "   Predicted Answer:  yes\n",
      "--------------------------------------------------\n",
      "12. Q: is a teal shape present?\n",
      "   Ground Truth:      no\n",
      "   Predicted Answer:  yes\n",
      "--------------------------------------------------\n",
      "13. Q: what color is the triangle?\n",
      "   Ground Truth:      blue\n",
      "   Predicted Answer:  blue\n",
      "--------------------------------------------------\n",
      "14. Q: what color is the shape?\n",
      "   Ground Truth:      blue\n",
      "   Predicted Answer:  blue\n",
      "--------------------------------------------------\n",
      "15. Q: is there not a rectangle in the image?\n",
      "   Ground Truth:      yes\n",
      "   Predicted Answer:  yes\n",
      "--------------------------------------------------\n",
      "16. Q: is there a red shape?\n",
      "   Ground Truth:      no\n",
      "   Predicted Answer:  no\n",
      "--------------------------------------------------\n",
      "17. Q: is there a green shape in the image?\n",
      "   Ground Truth:      no\n",
      "   Predicted Answer:  no\n",
      "--------------------------------------------------\n",
      "18. Q: is there not a teal shape?\n",
      "   Ground Truth:      yes\n",
      "   Predicted Answer:  yes\n",
      "--------------------------------------------------\n",
      "19. Q: what shape is in the image?\n",
      "   Ground Truth:      triangle\n",
      "   Predicted Answer:  triangle\n",
      "--------------------------------------------------\n",
      "20. Q: what shape does the image contain?\n",
      "   Ground Truth:      triangle\n",
      "   Predicted Answer:  triangle\n",
      "--------------------------------------------------\n",
      "21. Q: what color is the triangle?\n",
      "   Ground Truth:      red\n",
      "   Predicted Answer:  red\n",
      "--------------------------------------------------\n",
      "22. Q: what is the color of the triangle?\n",
      "   Ground Truth:      red\n",
      "   Predicted Answer:  red\n",
      "--------------------------------------------------\n",
      "23. Q: is there a circle?\n",
      "   Ground Truth:      no\n",
      "   Predicted Answer:  no\n",
      "--------------------------------------------------\n",
      "24. Q: does the image contain a green shape?\n",
      "   Ground Truth:      no\n",
      "   Predicted Answer:  no\n",
      "--------------------------------------------------\n",
      "25. Q: is there a blue shape in the image?\n",
      "   Ground Truth:      no\n",
      "   Predicted Answer:  no\n",
      "--------------------------------------------------\n",
      "26. Q: is there a teal shape?\n",
      "   Ground Truth:      no\n",
      "   Predicted Answer:  no\n",
      "--------------------------------------------------\n",
      "27. Q: is there not a teal shape in the image?\n",
      "   Ground Truth:      yes\n",
      "   Predicted Answer:  yes\n",
      "--------------------------------------------------\n",
      "28. Q: what shape is present?\n",
      "   Ground Truth:      rectangle\n",
      "   Predicted Answer:  square\n",
      "--------------------------------------------------\n",
      "29. Q: what shape does the image contain?\n",
      "   Ground Truth:      rectangle\n",
      "   Predicted Answer:  square\n",
      "--------------------------------------------------\n",
      "30. Q: what color is the rectangle?\n",
      "   Ground Truth:      brown\n",
      "   Predicted Answer:  red\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"First 30 Predictions:\\n\")\n",
    "\n",
    "for i, r in enumerate(results[:30]):\n",
    "    print(f\"{i+1}. Q: {r['question']}\")\n",
    "    print(f\"   Ground Truth:      {r['ground_truth']}\")\n",
    "    print(f\"   Predicted Answer:  {r['predicted_answer']}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f04c7b",
   "metadata": {},
   "source": [
    "ACCURACY= (correct x 100)/30=22x100/30= 73.33%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a5cb6e",
   "metadata": {},
   "source": [
    "Bonus: This images were too simple. But guess what? Salesforce has pretrained it's model on wide variety of images, so you can use it on any image of your choice.\n",
    "\n",
    "Try to load any real world image of any natural scene, or anything you like, and ask questions about it. You will be surprised by the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e32cb81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: how many mountains are there?\n",
      "A: 1\n"
     ]
    }
   ],
   "source": [
    "def ask_question_about_image(image_path, question):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(image, question, return_tensors=\"pt\")\n",
    "    output = model.generate(**inputs)\n",
    "    answer = processor.decode(output[0], skip_special_tokens=True)\n",
    "    return answer\n",
    "image_path = \"my_image.jpg\"  \n",
    "question =\"how many mountains are there?\"  \n",
    "\n",
    "answer = ask_question_about_image(image_path, question)\n",
    "print(f\"Q: {question}\")\n",
    "print(f\"A: {answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b83d31b",
   "metadata": {},
   "source": [
    "The End ðŸ«¡.\n",
    "\n",
    "If you have any questions, feel free to use following in the given order:\n",
    "- Google it!!\n",
    "- ChatGPT\n",
    "- Ask on the group"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
