{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05373b1b-ef95-449c-89c7-40069b5588ea",
   "metadata": {},
   "source": [
    "# Assignment 4: Text Classification on TREC dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2ad257-ec97-47bd-8cca-a2655fa5d92e",
   "metadata": {},
   "source": [
    "We are going to use the TREC dataset for this assignment, which is widely considered a benchmark text classification dataset. Read about the TREC dataset here (https://huggingface.co/datasets/CogComp/trec), also google it for understanding it better.\n",
    "\n",
    "This is what you have to do - use the concepts we have covered so far to accurately predict the 5 coarse labels (if you have googled TERC, you will surely know what I mean) in the test dataset. Train on the train dataset and give results on the test dataset, as simple as that. And experiment, experiment and experiment! \n",
    "\n",
    "Your experimentation should be 4-tiered-\n",
    "\n",
    "i) Experiment with preprocessing techniques (different types of Stemming, Lemmatizing, or do neither and keep the words pure). Needless to say, certain things, like stopword removal, should be common in all the preprocesssing pipelines you come up with. Remember never do stemming and lemmatization together. Note - To find out the best preprocessing technique, use a simple baseline model, like say CountVectorizer(BoW) + Logistic Regression, and see which gives the best accuracy. Then proceed with that preprocessing technique only for all the other models.\n",
    "\n",
    "ii) Try out various vectorisation techniques (BoW, TF-IDF, CBoW, Skipgram, GloVE, Fasttext, etc., but transformer models are not allowed) -- Atleast 5 different types\n",
    "\n",
    "iii) Tinker with various strategies to combine the word vectors (taking mean, using RNN/LSTM, and the other strategies I hinted at in the end of the last sesion). Note that this is applicable only for the advanced embedding techniques which generate word embeddings. -- Atleast 3 different types, one of which should definitely be RNN/LSTM\n",
    "\n",
    "iv) Finally, experiment with the ML classifier model, which will take the final vector respresentation of each TREC question and generate the label. E.g. - Logistic regression, decision trees, simple neural network, etc. - Atleast 4 different models\n",
    "\n",
    "So applying some PnC, in total you should get more than 40 different combinations. Print out the accuracies of all these combinations nicely in a well-formatted table, and pronounce one of them the best. Also feel free to experiment with more models/embedding techniques than what I have said here, the goal is after all to achieve the highest accuracy, as long as you don't use transformers. Happy experimenting!\n",
    "\n",
    "NOTE - While choosing the 4-5 types of each experimentation level, try to choose the best out of all those available. E.g. - For level (iii) - Tinker with various strategies to combine the word vectors - do not include 'mean' if you see it is giving horrendous results. Include the best 3-4 strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cca5a12-6ddf-4895-a962-fd8fac4ad1f9",
   "metadata": {},
   "source": [
    "### Helper Code to get you started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d08592-c633-4764-a60a-4937fd768cb4",
   "metadata": {},
   "source": [
    "I have added some helper code to show you how to load the TERC dataset and use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25cda3c-7d29-42c5-82b1-17ff2ac0d1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Question: How did serfdom develop in and then leave Russia ?\n",
      "Label: 2\n"
     ]
    }
   ],
   "source": [
    "%pip install -q datasets\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"trec\", trust_remote_code=True)\n",
    "train_data = dataset['train']\n",
    "test_data = dataset['test']\n",
    "\n",
    "print(\"Sample Question:\", train_data[0]['text'])\n",
    "print(\"Label:\", train_data[0]['coarse_label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d86430",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479cdc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "\n",
    "def preprocess(text, mode=\"pure\"):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    if mode == \"porter\":\n",
    "        tokens = [porter.stem(word) for word in tokens]\n",
    "    elif mode == \"snowball\":\n",
    "        tokens = [snowball.stem(word) for word in tokens]\n",
    "    elif mode == \"lemma\":\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e5e99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_raw = [x['text'] for x in train_data]\n",
    "y_train = [x['coarse_label'] for x in train_data]\n",
    "x_test_raw = [x['text'] for x in test_data]\n",
    "y_test = [x['coarse_label'] for x in test_data]\n",
    "\n",
    "results = []\n",
    "\n",
    "for mode in [\"pure\", \"porter\", \"snowball\", \"lemma\"]:\n",
    "\n",
    "    x_train = [preprocess(text, mode) for text in x_train_raw]\n",
    "    x_test = [preprocess(text, mode) for text in x_test_raw]\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    x_train_vec = vectorizer.fit_transform(x_train)\n",
    "    x_test_vec = vectorizer.transform(x_test)\n",
    "\n",
    "    clf = LogisticRegression(max_iter=300)\n",
    "    clf.fit(x_train_vec, y_train)\n",
    "    y_pred = clf.predict(x_test_vec)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    results.append((mode, acc))\n",
    "\n",
    "df_results = pd.DataFrame(results, columns=[\"Preprocessing\", \"Accuracy\"])\n",
    "df_results = df_results.sort_values(\"Accuracy\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47934e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "x_train = [preprocess(text, 'lemm') for text in x_train_raw]\n",
    "x_test = [preprocess(text, 'lemm') for text in x_test_raw]\n",
    "\n",
    "x_train_tok = [text.split() for text in x_train]\n",
    "x_test_tok = [text.split() for text in x_test]\n",
    "\n",
    "vectorized_data = {}\n",
    "\n",
    "vectorizer_bow = CountVectorizer()\n",
    "x_train_bow = vectorizer_bow.fit_transform(x_train)\n",
    "x_test_bow = vectorizer_bow.transform(x_test)\n",
    "vectorized_data['BoW'] = (x_train_bow, x_test_bow)\n",
    "\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "x_train_tfidf = vectorizer_tfidf.fit_transform(x_train)\n",
    "x_test_tfidf = vectorizer_tfidf.transform(x_test)\n",
    "vectorized_data['TF-IDF'] = (x_train_tfidf, x_test_tfidf)\n",
    "\n",
    "w2v_cbow = api.load(\"word2vec-google-news-300\")\n",
    "glove = api.load(\"glove-wiki-gigaword-300\")\n",
    "fasttext = api.load(\"fasttext-wiki-news-subwords-300\")\n",
    "w2v_skipgram = api.load(\"word2vec-ruscorpora-300\")\n",
    "\n",
    "def average_word_vectors(tokens_list, model, dim=300):\n",
    "    vectors = []\n",
    "    for tokens in tokens_list:\n",
    "        vecs = [model[word] for word in tokens if word in model]\n",
    "        if vecs:\n",
    "            vectors.append(np.mean(vecs, axis=0))\n",
    "        else:\n",
    "            vectors.append(np.zeros(dim))\n",
    "    return np.array(vectors)\n",
    "\n",
    "x_train_cbow = average_word_vectors(x_train_tok, w2v_cbow)\n",
    "x_test_cbow = average_word_vectors(x_test_tok, w2v_cbow)\n",
    "vectorized_data['Word2Vec_CBOW'] = (x_train_cbow, x_test_cbow)\n",
    "\n",
    "x_train_skip = average_word_vectors(x_train_tok, w2v_skipgram)\n",
    "x_test_skip = average_word_vectors(x_test_tok, w2v_skipgram)\n",
    "vectorized_data['Word2Vec_Skipgram'] = (x_train_skip, x_test_skip)\n",
    "\n",
    "x_train_glove = average_word_vectors(x_train_tok, glove)\n",
    "x_test_glove = average_word_vectors(x_test_tok, glove)\n",
    "vectorized_data['GloVe'] = (x_train_glove, x_test_glove)\n",
    "\n",
    "x_train_fast = average_word_vectors(x_train_tok, fasttext)\n",
    "x_test_fast = average_word_vectors(x_test_tok, fasttext)\n",
    "vectorized_data['FastText'] = (x_train_fast, x_test_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a995c64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorized_data(vectorizer_type):\n",
    "    if vectorizer_type == 'BoW':\n",
    "        return vectorized_data['BoW']\n",
    "    elif vectorizer_type == 'TF-IDF':\n",
    "        return vectorized_data['TF-IDF']\n",
    "    elif vectorizer_type == 'Word2Vec_CBOW':\n",
    "        return vectorized_data['Word2Vec_CBOW']\n",
    "    elif vectorizer_type == 'Word2Vec_Skipgram':\n",
    "        return vectorized_data['Word2Vec_Skipgram']\n",
    "    elif vectorizer_type == 'GloVe':\n",
    "        return vectorized_data['GloVe']\n",
    "    elif vectorizer_type == 'FastText':\n",
    "        return vectorized_data['FastText']\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown vectorizer type: {vectorizer_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a47aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_mean(embeddings_list, model, dim=300):\n",
    "    vectors = []\n",
    "    for tokens in embeddings_list:\n",
    "        vecs = [model[word] for word in tokens if word in model]\n",
    "        if vecs:\n",
    "            vectors.append(np.mean(vecs, axis=0))\n",
    "        else:\n",
    "            vectors.append(np.zeros(dim))\n",
    "    return np.array(vectors)\n",
    "\n",
    "def combine_max(embeddings_list, model, dim=300):\n",
    "    vectors = []\n",
    "    for tokens in embeddings_list:\n",
    "        vecs = [model[word] for word in tokens if word in model]\n",
    "        if vecs:\n",
    "            vectors.append(np.max(vecs, axis=0))\n",
    "        else:\n",
    "            vectors.append(np.zeros(dim))\n",
    "    return np.array(vectors)\n",
    "\n",
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=300, hidden_dim=128):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hn, _) = self.lstm(x)\n",
    "        return hn.squeeze(0)\n",
    "\n",
    "def pad_sequences_token_vectors(token_lists, model, dim=300, max_len=40):\n",
    "    padded = []\n",
    "    for tokens in token_lists:\n",
    "        vecs = [model[word] for word in tokens if word in model]\n",
    "        if not vecs:\n",
    "            vecs = [np.zeros(dim)]\n",
    "        if len(vecs) > max_len:\n",
    "            vecs = vecs[:max_len]\n",
    "        else:\n",
    "            vecs += [np.zeros(dim)] * (max_len - len(vecs))\n",
    "        padded.append(vecs)\n",
    "    return torch.tensor(padded, dtype=torch.float32)\n",
    "\n",
    "def combine_lstm(embeddings_list, model, dim=300, max_len=40, batch_size=128):\n",
    "    lstm_model = LSTMEncoder(input_dim=dim)\n",
    "    lstm_model.eval()\n",
    "\n",
    "    data_tensor = pad_sequences_token_vectors(embeddings_list, model, dim, max_len)\n",
    "    loader = DataLoader(data_tensor, batch_size=batch_size)\n",
    "\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            encoded = lstm_model(batch)\n",
    "            outputs.append(encoded)\n",
    "    return torch.cat(outputs).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1d08c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_vectors(method, token_list, model, dim=300):\n",
    "    if method == 'mean':\n",
    "        return combine_mean(token_list, model, dim)\n",
    "    elif method == 'max':\n",
    "        return combine_max(token_list, model, dim)\n",
    "    elif method == 'lstm':\n",
    "        return combine_lstm(token_list, model, dim)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown combiner method: {method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c9c7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_classifier(x_train_vec, x_test_vec, y_train, y_test, classifier_type):\n",
    "    if classifier_type == 'LogisticRegression':\n",
    "        clf = LogisticRegression(max_iter=300)\n",
    "    elif classifier_type == 'DecisionTree':\n",
    "        clf = DecisionTreeClassifier()\n",
    "    elif classifier_type == 'RandomForest':\n",
    "        clf = RandomForestClassifier(n_estimators=100)\n",
    "    elif classifier_type == 'MLP':\n",
    "        clf = MLPClassifier(hidden_layer_sizes=(128,), max_iter=300)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown classifier type: {classifier_type}\")\n",
    "\n",
    "    clf.fit(x_train_vec, y_train)\n",
    "    y_pred = clf.predict(x_test_vec)\n",
    "    return accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d17af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "def log_result(preprocessing, vectorization, combiner, classifier, accuracy):\n",
    "    results.append({\n",
    "        'Preprocessing': preprocessing,\n",
    "        'Vectorization': vectorization,\n",
    "        'Combiner': combiner,\n",
    "        'Classifier': classifier,\n",
    "        'Accuracy': accuracy\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d1b6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "classifier_types = ['LogisticRegression', 'DecisionTree', 'RandomForest', 'MLP']\n",
    "vectorizer_types = list(vectorized_data.keys())\n",
    "\n",
    "for vec_type in vectorizer_types:\n",
    "    x_train_vec, x_test_vec = get_vectorized_data(vec_type)\n",
    "\n",
    "    if vec_type in ['BoW', 'TF-IDF']:\n",
    "        combiner = 'N/A'\n",
    "    else:\n",
    "        combiner = 'mean'\n",
    "\n",
    "    for clf_type in classifier_types:\n",
    "        acc = train_and_evaluate_classifier(x_train_vec, x_test_vec, y_train, y_test, clf_type)\n",
    "        log_result('lemma', vec_type, combiner, clf_type, acc)\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results = df_results.sort_values(\"Accuracy\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
